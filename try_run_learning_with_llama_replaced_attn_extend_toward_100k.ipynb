{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "711813d7-9d3e-43e6-a87b-9621b1d1af95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:37:26.392003Z",
     "iopub.status.busy": "2024-05-10T13:37:26.391582Z",
     "iopub.status.idle": "2024-05-10T13:37:28.348648Z",
     "shell.execute_reply": "2024-05-10T13:37:28.347995Z",
     "shell.execute_reply.started": "2024-05-10T13:37:26.391983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbae624-3227-4e4d-9555-5543a11ff4f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T02:27:22.236810Z",
     "iopub.status.busy": "2024-05-10T02:27:22.235886Z",
     "iopub.status.idle": "2024-05-10T02:30:42.668347Z",
     "shell.execute_reply": "2024-05-10T02:30:42.667574Z",
     "shell.execute_reply.started": "2024-05-10T02:27:22.236781Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 02:27:29.035552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Written by Yukang Chen\n",
    "# Some code based on https://github.com/epfml/landmark-attention\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "from llama_attn_replace_sft import replace_llama_attn\n",
    "from gptneox_attn_replace import replace_gpt_neox_attn\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.distributed import barrier\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input_llama2\":(\n",
    "        \"[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\"\n",
    "        \"<</SYS>> \\n\\n {instruction} [/INST]\"\n",
    "    ),\n",
    "    \"prompt_input_llama2\": (\n",
    "        \"[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\"\n",
    "        \"<</SYS>> \\n\\n {instruction} \\n{input} [/INST]\"\n",
    "    ),\n",
    "    \"prompt_llama2\": \"[INST]{instruction}[/INST]\",\n",
    "    \"prompt_input_diploma_special\":(\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\nBelow is a diploma text. Your task is to generate abstract of this diploma.\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"EleutherAI/pythia-1.4b-deduped\")\n",
    "    model_type: Optional[str] = field(default=\"llama\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    train_data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\n",
    "    val_data_path: str = field(default=None, metadata={\"help\": \"Path to the validation data.\"})\n",
    "    nrows: int = 1\n",
    "    diploma_prefix_len: int = 1\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=8192 * 4,\n",
    "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
    "    )\n",
    "    use_flash_attn: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether use flash attention for training.\"},\n",
    "    )\n",
    "    use_full_attn: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use plain, full-attention for training.\"},\n",
    "    )\n",
    "    low_rank_training: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether use low rank adaptation for training.\"},\n",
    "    )\n",
    "    trainable_params: str = field(\n",
    "        default=\"embed,norm\",\n",
    "        metadata={\"help\": \"Additional trainable parameters except LoRA weights, if low rank training.\"},\n",
    "    )\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, nrows: int, diploma_prefix_len: int):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        data_table = pd.read_csv(data_path)\n",
    "        data_table = data_table.sample(min(len(data_table), nrows))\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "\n",
    "        prompt_input_diploma = PROMPT_DICT[\"prompt_input_diploma_special\"]\n",
    "        sources = [\n",
    "            prompt_input_diploma.format(input=diploma[:diploma_prefix_len])\n",
    "            for diploma in data_table[\"diploma\"]\n",
    "        ]\n",
    "\n",
    "        targets = [f\"{abstract}{tokenizer.eos_token}\" for abstract in data_table[\"abstract\"]]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.train_data_path, nrows=data_args.nrows, diploma_prefix_len=data_args.diploma_prefix_len)\n",
    "    val_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.val_data_path, nrows=data_args.nrows, diploma_prefix_len=data_args.diploma_prefix_len)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=val_dataset, data_collator=data_collator)\n",
    "\n",
    "\n",
    "def train(model_args, data_args, training_args):\n",
    "    print(\"Begin train\")\n",
    "    \n",
    "    print(\"Parsed arguments\")\n",
    "\n",
    "    # NOTE: May expand supported model types in the future\n",
    "    if model_args.model_type == \"gpt-neox\":\n",
    "        replace_gpt_neox_attn(training_args.use_flash_attn, training_args.use_full_attn)\n",
    "    else:\n",
    "        replace_llama_attn(training_args.use_flash_attn, training_args.use_full_attn)\n",
    "\n",
    "    # Set RoPE scaling factor\n",
    "    config = transformers.AutoConfig.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "\n",
    "    orig_rope_scaling = getattr(config, \"rope_scaling\", None)\n",
    "    if orig_rope_scaling is None:\n",
    "        orig_rope_scaling = {\"factor\": 1}\n",
    "    orig_rope_scaling_factor = orig_rope_scaling[\"factor\"] if \"factor\" in orig_rope_scaling.keys() else 1\n",
    "    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "    if orig_ctx_len:\n",
    "        orig_ctx_len *= orig_rope_scaling_factor\n",
    "        if training_args.model_max_length > orig_ctx_len:\n",
    "            scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n",
    "            config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "            \n",
    "    print(\"Created config\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    print(\"Loaded model\")\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Loaded tokenizer\")\n",
    "\n",
    "    special_tokens_dict = dict()\n",
    "    if tokenizer.pad_token is None:\n",
    "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "    if tokenizer.unk_token is None:\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "    smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict=special_tokens_dict,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "    \n",
    "    print(\"Created data_module\")\n",
    "\n",
    "    if training_args.low_rank_training:\n",
    "        if model_args.model_type == \"gpt-neox\":\n",
    "            # added `dense` to match with llama as the basic LoRA would only target 'query_key_value'\n",
    "            targets = [\"query_key_value\", \"dense\"]\n",
    "        else:\n",
    "            targets=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "        config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=targets,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        # enable trainable params\n",
    "        [p.requires_grad_() for n, p in model.named_parameters() if any([k in n for k in training_args.trainable_params.split(\",\")])]\n",
    "\n",
    "    model.config.use_cache = False         # required for gradient checkpointing\n",
    "    model.enable_input_require_grads()     # required for gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()  # enable gradient checkpointing\n",
    "    \n",
    "    print(\"Prepared model to learn\")\n",
    "\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "    trainer.train()\n",
    "    trainer.save_state()\n",
    "    trainer.save_model(output_dir=training_args.output_dir)\n",
    "    \n",
    "    print(\"Learnt model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf489992-f0e8-499d-a4a7-f464cdd25bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T02:27:17.937555Z",
     "iopub.status.busy": "2024-05-10T02:27:17.937177Z",
     "iopub.status.idle": "2024-05-10T02:27:18.083713Z",
     "shell.execute_reply": "2024-05-10T02:27:18.083043Z",
     "shell.execute_reply.started": "2024-05-10T02:27:17.937535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46a1c82669548fdaf531e48f225087f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e59a20-46e8-4078-a3ad-afb93f3fa2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:21:51.988770Z",
     "iopub.status.busy": "2024-05-10T00:21:51.988390Z",
     "iopub.status.idle": "2024-05-10T00:21:52.010967Z",
     "shell.execute_reply": "2024-05-10T00:21:52.010250Z",
     "shell.execute_reply.started": "2024-05-10T00:21:51.988751Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--meta-llama--Llama-2-7b-hf\n",
      "tmpvpe__rmk\n",
      "tmpyhzuk13t\n"
     ]
    }
   ],
   "source": [
    "! ls ../cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fbd9ad8-8d21-4bbb-9a75-be473d68b3be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T02:30:42.669906Z",
     "iopub.status.busy": "2024-05-10T02:30:42.669378Z",
     "iopub.status.idle": "2024-05-10T02:30:42.678279Z",
     "shell.execute_reply": "2024-05-10T02:30:42.677696Z",
     "shell.execute_reply.started": "2024-05-10T02:30:42.669885Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "CACHE_DIR = Path(\"../cache/\")\n",
    "DATASET_DIR = Path(\"/home/jupyter/mnt/datasets/diplomas/russian_dataset/\")\n",
    "OUTPUT_DIR = Path(\"./llama_replaced_attn_64k_without_checkpoints_output_dir/\")\n",
    "LOGGING_DIR = Path(\"./llama_replaced_attn_64k_without_checkpoints_logging_dir/\")\n",
    "MODEL_MAX_LENGTH = 64000\n",
    "INF = int(1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c7a875-15a8-4b69-8899-90056fddfbbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T02:30:42.679691Z",
     "iopub.status.busy": "2024-05-10T02:30:42.679422Z",
     "iopub.status.idle": "2024-05-10T02:30:42.710739Z",
     "shell.execute_reply": "2024-05-10T02:30:42.710135Z",
     "shell.execute_reply.started": "2024-05-10T02:30:42.679674Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir='llama_replaced_attn_64k_without_checkpoints_output_dir', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5, max_steps=-1, lr_scheduler_type=<SchedulerType.CONSTANT_WITH_WARMUP: 'constant_with_warmup'>, warmup_ratio=0.0, warmup_steps=20, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='llama_replaced_attn_64k_without_checkpoints_logging_dir', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=1, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.NO: 'no'>, save_steps=500, save_total_limit=None, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=True, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, past_index=-1, run_name='llama_replaced_attn_64k_without_checkpoints_output_dir', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed='ds_configs/stage2.json', label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, cache_dir='../cache', model_max_length=64000, use_flash_attn=True, use_full_attn=False, low_rank_training=True, trainable_params='embed,norm')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\",\n",
    ")\n",
    "\n",
    "\n",
    "data_args = DataArguments(\n",
    "    train_data_path=DATASET_DIR.joinpath(\"russian_dataset_train.csv\").as_posix(),\n",
    "    val_data_path=DATASET_DIR.joinpath(\"russian_dataset_val.csv\").as_posix(),\n",
    "    nrows=720 * 3,\n",
    "    diploma_prefix_len=INF,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    bf16=True,\n",
    "    output_dir=OUTPUT_DIR.as_posix(),\n",
    "    model_max_length=MODEL_MAX_LENGTH,\n",
    "    use_flash_attn=True,\n",
    "    low_rank_training=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=20,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    logging_steps=1,\n",
    "    logging_dir=LOGGING_DIR.as_posix(),\n",
    "    deepspeed=\"ds_configs/stage2.json\",\n",
    "    tf32=True,\n",
    "    cache_dir=CACHE_DIR.as_posix(),\n",
    "    report_to=['tensorboard'],\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d13ac05-59df-4158-9430-7dadcd12207d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:08:34.082275Z",
     "iopub.status.busy": "2024-05-10T13:08:34.081869Z",
     "iopub.status.idle": "2024-05-10T13:08:34.097545Z",
     "shell.execute_reply": "2024-05-10T13:08:34.096849Z",
     "shell.execute_reply.started": "2024-05-10T13:08:34.082255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.use_flash_attn, training_args.use_full_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3004f96c-a627-4849-a441-7d8a3aacbfd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T02:30:42.712264Z",
     "iopub.status.busy": "2024-05-10T02:30:42.711831Z",
     "iopub.status.idle": "2024-05-10T03:40:31.225452Z",
     "shell.execute_reply": "2024-05-10T03:40:31.224291Z",
     "shell.execute_reply.started": "2024-05-10T02:30:42.712242Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin train\n",
      "Parsed arguments\n",
      "Created config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data_module\n",
      "Prepared model to learn\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin train\")\n",
    "\n",
    "print(\"Parsed arguments\")\n",
    "\n",
    "# NOTE: May expand supported model types in the future\n",
    "if model_args.model_type == \"gpt-neox\":\n",
    "    replace_gpt_neox_attn(training_args.use_flash_attn, training_args.use_full_attn)\n",
    "else:\n",
    "    replace_llama_attn(training_args.use_flash_attn, training_args.use_full_attn)\n",
    "\n",
    "# Set RoPE scaling factor\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    ")\n",
    "\n",
    "orig_rope_scaling = getattr(config, \"rope_scaling\", None)\n",
    "if orig_rope_scaling is None:\n",
    "    orig_rope_scaling = {\"factor\": 1}\n",
    "orig_rope_scaling_factor = orig_rope_scaling[\"factor\"] if \"factor\" in orig_rope_scaling.keys() else 1\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len:\n",
    "    orig_ctx_len *= orig_rope_scaling_factor\n",
    "    if training_args.model_max_length > orig_ctx_len:\n",
    "        scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n",
    "        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "\n",
    "print(\"Created config\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "print(\"Loaded tokenizer\")\n",
    "\n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=special_tokens_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "\n",
    "print(\"Created data_module\")\n",
    "\n",
    "if training_args.low_rank_training:\n",
    "    if model_args.model_type == \"gpt-neox\":\n",
    "        # added `dense` to match with llama as the basic LoRA would only target 'query_key_value'\n",
    "        targets = [\"query_key_value\", \"dense\"]\n",
    "    else:\n",
    "        targets=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=targets,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    # enable trainable params\n",
    "    [p.requires_grad_() for n, p in model.named_parameters() if any([k in n for k in training_args.trainable_params.split(\",\")])]\n",
    "\n",
    "model.config.use_cache = False         # required for gradient checkpointing\n",
    "model.enable_input_require_grads()     # required for gradient checkpointing\n",
    "model.gradient_checkpointing_enable()  # enable gradient checkpointing\n",
    "\n",
    "print(\"Prepared model to learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa413f50-310a-4ecc-b73f-79805f3a06d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T03:40:31.226507Z",
     "iopub.status.busy": "2024-05-10T03:40:31.226241Z",
     "iopub.status.idle": "2024-05-10T03:40:31.244663Z",
     "shell.execute_reply": "2024-05-10T03:40:31.244055Z",
     "shell.execute_reply.started": "2024-05-10T03:40:31.226488Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK ABOUT TOKENIZING LARGE DIPLOMAS: DOES THERE LABELS OK?\n",
    "list(set(data_module[\"train_dataset\"][9][\"labels\"].tolist())).__len__() > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7f5c61-ccee-4e28-9971-71cf6af6c86b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T03:40:31.247192Z",
     "iopub.status.busy": "2024-05-10T03:40:31.246933Z",
     "iopub.status.idle": "2024-05-10T03:40:31.270068Z",
     "shell.execute_reply": "2024-05-10T03:40:31.269205Z",
     "shell.execute_reply.started": "2024-05-10T03:40:31.247175Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44081.64953703704"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "shapes = []\n",
    "for x in data_module[\"train_dataset\"]:\n",
    "    shapes.append(x[\"input_ids\"].shape[0])\n",
    "np.mean(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08f39a7f-3934-4639-9cd3-991e9da71fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T03:40:31.271718Z",
     "iopub.status.busy": "2024-05-10T03:40:31.271374Z",
     "iopub.status.idle": "2024-05-10T12:37:04.208871Z",
     "shell.execute_reply": "2024-05-10T12:37:04.207289Z",
     "shell.execute_reply.started": "2024-05-10T03:40:31.271698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 03:40:31,660] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The default cache directory for DeepSpeed Triton autotune, /home/jupyter/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1350 [01:46<39:46:59, 106.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.0658, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1350 [03:25<38:19:42, 102.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.6262, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1350 [05:20<40:23:38, 107.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 46.4006, 'learning_rate': 3e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1350 [07:09<40:33:18, 108.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.3714, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1350 [08:39<37:58:58, 101.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.4604, 'learning_rate': 5e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1350 [10:34<39:38:56, 106.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.879, 'learning_rate': 6e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1350 [12:00<37:13:01, 99.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.2829, 'learning_rate': 7e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1350 [14:08<40:28:38, 108.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 27.4258, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1350 [15:51<39:46:02, 106.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 24.7523, 'learning_rate': 9e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1350 [17:37<39:40:57, 106.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 37.1097, 'learning_rate': 1e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1350 [19:12<38:20:31, 103.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.6729, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1350 [20:51<37:52:22, 101.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 20.4278, 'learning_rate': 1.2e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/1350 [22:28<37:14:41, 100.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.1913, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1350 [23:47<34:49:20, 93.83s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.8693, 'learning_rate': 1.4e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/1350 [25:53<38:25:00, 103.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 39.2168, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/1350 [27:30<37:38:40, 101.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8809, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 17/1350 [29:25<39:05:18, 105.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.4842, 'learning_rate': 1.7e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 18/1350 [31:05<38:31:31, 104.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5759, 'learning_rate': 1.8e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 19/1350 [32:36<37:03:01, 100.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.1932, 'learning_rate': 1.9e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 20/1350 [34:40<39:35:28, 107.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 20.0782, 'learning_rate': 2e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1350 [36:00<36:36:53, 99.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.1086, 'learning_rate': 2e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/1350 [37:41<36:44:06, 99.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.8875, 'learning_rate': 2e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1350 [39:22<36:52:40, 100.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.9824, 'learning_rate': 2e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1350 [41:13<38:04:05, 103.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 20.6625, 'learning_rate': 2e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 25/1350 [43:19<40:32:36, 110.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 27.3012, 'learning_rate': 2e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 26/1350 [45:04<39:55:59, 108.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9303, 'learning_rate': 2e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 27/1350 [46:28<37:12:32, 101.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.5611, 'learning_rate': 2e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 28/1350 [48:08<37:01:14, 100.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.7477, 'learning_rate': 2e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 29/1350 [50:05<38:47:10, 105.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.3022, 'learning_rate': 2e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 30/1350 [51:48<38:24:55, 104.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.2716, 'learning_rate': 2e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 31/1350 [53:21<37:08:01, 101.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.2661, 'learning_rate': 2e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 32/1350 [54:56<36:25:00, 99.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5256, 'learning_rate': 2e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 33/1350 [56:40<36:53:11, 100.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 17.1077, 'learning_rate': 2e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 34/1350 [58:26<37:24:30, 102.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.7475, 'learning_rate': 2e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 35/1350 [1:00:38<40:39:16, 111.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6755, 'learning_rate': 2e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 36/1350 [1:02:40<41:45:52, 114.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6699, 'learning_rate': 2e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 37/1350 [1:04:14<39:30:28, 108.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8521, 'learning_rate': 2e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 38/1350 [1:05:59<39:06:17, 107.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.5422, 'learning_rate': 2e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 39/1350 [1:07:21<36:22:15, 99.87s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5824, 'learning_rate': 2e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 40/1350 [1:08:57<35:49:57, 98.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.807, 'learning_rate': 2e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 41/1350 [1:10:40<36:20:55, 99.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3906, 'learning_rate': 2e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 42/1350 [1:12:29<37:20:23, 102.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6246, 'learning_rate': 2e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 43/1350 [1:14:02<36:14:15, 99.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.8435, 'learning_rate': 2e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 44/1350 [1:15:53<37:20:45, 102.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2206, 'learning_rate': 2e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 45/1350 [1:17:30<36:40:03, 101.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4471, 'learning_rate': 2e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 46/1350 [1:19:28<38:31:35, 106.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.2448, 'learning_rate': 2e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 47/1350 [1:21:20<39:06:11, 108.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0853, 'learning_rate': 2e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 48/1350 [1:23:01<38:16:04, 105.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5448, 'learning_rate': 2e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 49/1350 [1:24:42<37:48:06, 104.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0525, 'learning_rate': 2e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 50/1350 [1:26:29<37:58:21, 105.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.8509, 'learning_rate': 2e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 51/1350 [1:28:18<38:24:46, 106.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6802, 'learning_rate': 2e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 52/1350 [1:30:09<38:52:03, 107.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.4742, 'learning_rate': 2e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 53/1350 [1:31:48<37:54:29, 105.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9362, 'learning_rate': 2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 54/1350 [1:33:39<38:24:21, 106.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.0471, 'learning_rate': 2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 55/1350 [1:35:40<39:56:21, 111.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.9961, 'learning_rate': 2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 56/1350 [1:37:29<39:45:49, 110.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.4716, 'learning_rate': 2e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 57/1350 [1:39:36<41:26:15, 115.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.3782, 'learning_rate': 2e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 58/1350 [1:41:02<38:13:13, 106.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6607, 'learning_rate': 2e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 59/1350 [1:42:47<38:07:01, 106.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.6097, 'learning_rate': 2e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 60/1350 [1:44:28<37:31:02, 104.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9684, 'learning_rate': 2e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 61/1350 [1:46:10<37:11:03, 103.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4844, 'learning_rate': 2e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 62/1350 [1:47:39<35:29:00, 99.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.1337, 'learning_rate': 2e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 63/1350 [1:49:11<34:42:10, 97.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7547, 'learning_rate': 2e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 64/1350 [1:50:41<33:55:13, 94.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.1041, 'learning_rate': 2e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 65/1350 [1:52:29<35:19:52, 98.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.9828, 'learning_rate': 2e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 66/1350 [1:54:26<37:10:25, 104.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.8323, 'learning_rate': 2e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 67/1350 [1:56:17<37:55:38, 106.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.0883, 'learning_rate': 2e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 68/1350 [1:57:38<35:10:21, 98.77s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.836, 'learning_rate': 2e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 69/1350 [1:59:19<35:22:20, 99.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.3785, 'learning_rate': 2e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 70/1350 [2:00:46<34:03:14, 95.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0839, 'learning_rate': 2e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 71/1350 [2:02:25<34:17:55, 96.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2407, 'learning_rate': 2e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 72/1350 [2:04:19<36:07:16, 101.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.429, 'learning_rate': 2e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 73/1350 [2:05:48<34:44:53, 97.96s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7942, 'learning_rate': 2e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 74/1350 [2:07:04<32:27:13, 91.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2051, 'learning_rate': 2e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 75/1350 [2:08:41<33:01:36, 93.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6448, 'learning_rate': 2e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 76/1350 [2:10:28<34:22:53, 97.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5892, 'learning_rate': 2e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 77/1350 [2:12:28<36:50:01, 104.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2046, 'learning_rate': 2e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 78/1350 [2:14:32<38:54:53, 110.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.0187, 'learning_rate': 2e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 79/1350 [2:16:33<39:57:22, 113.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1272, 'learning_rate': 2e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 80/1350 [2:18:13<38:33:45, 109.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.0461, 'learning_rate': 2e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 81/1350 [2:19:36<35:45:52, 101.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.2819, 'learning_rate': 2e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 82/1350 [2:20:56<33:31:04, 95.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9144, 'learning_rate': 2e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 83/1350 [2:23:04<36:51:40, 104.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.3873, 'learning_rate': 2e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 84/1350 [2:25:15<39:35:54, 112.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6567, 'learning_rate': 2e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 85/1350 [2:27:11<39:56:03, 113.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0273, 'learning_rate': 2e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 86/1350 [2:29:03<39:43:54, 113.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4424, 'learning_rate': 2e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 87/1350 [2:30:32<37:11:44, 106.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.2996, 'learning_rate': 2e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 88/1350 [2:32:09<36:11:33, 103.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8431, 'learning_rate': 2e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 89/1350 [2:34:01<37:09:39, 106.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2434, 'learning_rate': 2e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 90/1350 [2:36:04<38:50:42, 110.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4611, 'learning_rate': 2e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 91/1350 [2:37:40<37:17:31, 106.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6594, 'learning_rate': 2e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 92/1350 [2:39:42<38:52:10, 111.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.1006, 'learning_rate': 2e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 93/1350 [2:41:37<39:14:32, 112.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.4969, 'learning_rate': 2e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 94/1350 [2:43:44<40:39:14, 116.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2105, 'learning_rate': 2e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 95/1350 [2:45:34<39:58:13, 114.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.3897, 'learning_rate': 2e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 96/1350 [2:47:20<39:01:46, 112.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6073, 'learning_rate': 2e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 97/1350 [2:48:48<36:32:16, 104.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8851, 'learning_rate': 2e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 98/1350 [2:50:39<37:06:45, 106.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.1523, 'learning_rate': 2e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 99/1350 [2:52:35<38:02:08, 109.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.3189, 'learning_rate': 2e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 100/1350 [2:54:25<38:06:58, 109.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.3855, 'learning_rate': 2e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 101/1350 [2:56:01<36:36:44, 105.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8467, 'learning_rate': 2e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 102/1350 [2:58:05<38:28:26, 110.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4867, 'learning_rate': 2e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 103/1350 [2:59:36<36:25:41, 105.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9138, 'learning_rate': 2e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 104/1350 [3:01:12<35:24:45, 102.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.0918, 'learning_rate': 2e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 105/1350 [3:02:44<34:19:13, 99.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6976, 'learning_rate': 2e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 106/1350 [3:04:18<33:44:06, 97.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4819, 'learning_rate': 2e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 107/1350 [3:05:55<33:39:15, 97.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9048, 'learning_rate': 2e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 108/1350 [3:07:40<34:25:58, 99.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.248, 'learning_rate': 2e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 109/1350 [3:09:04<32:42:52, 94.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5561, 'learning_rate': 2e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 110/1350 [3:10:17<30:28:25, 88.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2905, 'learning_rate': 2e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 111/1350 [3:12:15<33:29:55, 97.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.5656, 'learning_rate': 2e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 112/1350 [3:14:04<34:39:48, 100.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.1469, 'learning_rate': 2e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 113/1350 [3:16:06<36:46:12, 107.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2284, 'learning_rate': 2e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 114/1350 [3:17:55<36:56:20, 107.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.496, 'learning_rate': 2e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 115/1350 [3:19:50<37:41:42, 109.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 20.8525, 'learning_rate': 2e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 116/1350 [3:21:13<34:55:51, 101.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2729, 'learning_rate': 2e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 117/1350 [3:23:02<35:36:27, 103.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2765, 'learning_rate': 2e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 118/1350 [3:24:59<36:57:49, 108.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1562, 'learning_rate': 2e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 119/1350 [3:26:51<37:16:55, 109.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.9591, 'learning_rate': 2e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 120/1350 [3:29:01<39:23:30, 115.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2589, 'learning_rate': 2e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 121/1350 [3:30:52<38:55:20, 114.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.8936, 'learning_rate': 2e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 122/1350 [3:32:43<38:35:26, 113.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3648, 'learning_rate': 2e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 123/1350 [3:34:10<35:52:14, 105.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2227, 'learning_rate': 2e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 124/1350 [3:36:04<36:49:13, 108.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8129, 'learning_rate': 2e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 125/1350 [3:37:52<36:43:39, 107.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.1845, 'learning_rate': 2e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 126/1350 [3:39:29<35:35:56, 104.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9345, 'learning_rate': 2e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 127/1350 [3:41:01<34:16:27, 100.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6689, 'learning_rate': 2e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 128/1350 [3:42:39<33:54:55, 99.91s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.9769, 'learning_rate': 2e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 129/1350 [3:44:19<33:54:34, 99.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9946, 'learning_rate': 2e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 130/1350 [3:46:13<35:21:19, 104.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.7634, 'learning_rate': 2e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 131/1350 [3:47:31<32:35:40, 96.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8224, 'learning_rate': 2e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 132/1350 [3:49:05<32:24:14, 95.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0245, 'learning_rate': 2e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 133/1350 [3:50:30<31:14:56, 92.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0809, 'learning_rate': 2e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 134/1350 [3:52:06<31:36:48, 93.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2838, 'learning_rate': 2e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 135/1350 [3:54:05<34:07:33, 101.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 26.8229, 'learning_rate': 2e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 136/1350 [3:56:17<37:12:43, 110.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3548, 'learning_rate': 2e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 137/1350 [3:58:03<36:44:22, 109.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.6559, 'learning_rate': 2e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 138/1350 [3:59:47<36:11:38, 107.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.6101, 'learning_rate': 2e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 139/1350 [4:01:40<36:46:01, 109.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5497, 'learning_rate': 2e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 140/1350 [4:03:32<36:57:02, 109.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.3406, 'learning_rate': 2e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 141/1350 [4:05:05<35:13:46, 104.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4034, 'learning_rate': 2e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 142/1350 [4:06:31<33:20:08, 99.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.4414, 'learning_rate': 2e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 143/1350 [4:08:08<33:03:47, 98.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6196, 'learning_rate': 2e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 144/1350 [4:09:45<32:50:32, 98.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.082, 'learning_rate': 2e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 145/1350 [4:11:08<31:17:35, 93.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8301, 'learning_rate': 2e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 146/1350 [4:12:59<33:01:17, 98.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1679, 'learning_rate': 2e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 147/1350 [4:14:36<32:53:12, 98.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.089, 'learning_rate': 2e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 148/1350 [4:16:14<32:44:07, 98.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.062, 'learning_rate': 2e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 149/1350 [4:18:32<36:47:00, 110.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3533, 'learning_rate': 2e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 150/1350 [4:20:27<37:09:32, 111.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3175, 'learning_rate': 2e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 151/1350 [4:22:17<36:59:57, 111.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2503, 'learning_rate': 2e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 152/1350 [4:24:08<37:01:08, 111.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7014, 'learning_rate': 2e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 153/1350 [4:26:03<37:19:05, 112.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2497, 'learning_rate': 2e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 154/1350 [4:27:24<34:09:19, 102.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9518, 'learning_rate': 2e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 155/1350 [4:29:03<33:47:12, 101.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6954, 'learning_rate': 2e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 156/1350 [4:31:02<35:24:48, 106.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.2954, 'learning_rate': 2e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 157/1350 [4:33:01<36:40:36, 110.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.3808, 'learning_rate': 2e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 158/1350 [4:34:34<34:48:28, 105.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2516, 'learning_rate': 2e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 159/1350 [4:36:12<34:09:46, 103.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1197, 'learning_rate': 2e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 160/1350 [4:38:07<35:16:54, 106.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8979, 'learning_rate': 2e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 161/1350 [4:40:24<38:12:14, 115.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.4318, 'learning_rate': 2e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 162/1350 [4:42:34<39:36:23, 120.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.2457, 'learning_rate': 2e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 163/1350 [4:44:25<38:39:01, 117.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1812, 'learning_rate': 2e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 164/1350 [4:46:20<38:24:56, 116.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6262, 'learning_rate': 2e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 165/1350 [4:48:04<37:06:39, 112.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.159, 'learning_rate': 2e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 166/1350 [4:50:01<37:30:22, 114.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 19.3638, 'learning_rate': 2e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 167/1350 [4:51:41<36:09:46, 110.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5198, 'learning_rate': 2e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 168/1350 [4:53:31<36:02:38, 109.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.54, 'learning_rate': 2e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 169/1350 [4:55:07<34:44:11, 105.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2972, 'learning_rate': 2e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 170/1350 [4:56:53<34:42:15, 105.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1448, 'learning_rate': 2e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 171/1350 [4:58:30<33:47:43, 103.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1494, 'learning_rate': 2e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 172/1350 [5:00:22<34:35:14, 105.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9807, 'learning_rate': 2e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 173/1350 [5:01:41<31:59:47, 97.87s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0255, 'learning_rate': 2e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 174/1350 [5:03:22<32:12:12, 98.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0021, 'learning_rate': 2e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 175/1350 [5:05:01<32:15:46, 98.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7223, 'learning_rate': 2e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 176/1350 [5:07:15<35:41:14, 109.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.73, 'learning_rate': 2e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 177/1350 [5:08:23<31:36:34, 97.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4592, 'learning_rate': 2e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 178/1350 [5:10:31<34:32:54, 106.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3621, 'learning_rate': 2e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 179/1350 [5:12:13<34:09:24, 105.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2107, 'learning_rate': 2e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 180/1350 [5:13:46<32:55:58, 101.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.3596, 'learning_rate': 2e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 181/1350 [5:15:05<30:46:04, 94.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8658, 'learning_rate': 2e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 182/1350 [5:17:02<32:53:07, 101.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4709, 'learning_rate': 2e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 183/1350 [5:18:59<34:24:33, 106.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7989, 'learning_rate': 2e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 184/1350 [5:20:49<34:47:04, 107.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7883, 'learning_rate': 2e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 185/1350 [5:22:44<35:25:53, 109.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 26.9506, 'learning_rate': 2e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 186/1350 [5:24:37<35:42:44, 110.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8069, 'learning_rate': 2e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 187/1350 [5:26:28<35:49:08, 110.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.683, 'learning_rate': 2e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 188/1350 [5:28:31<36:56:15, 114.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0195, 'learning_rate': 2e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 189/1350 [5:30:13<35:40:31, 110.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.484, 'learning_rate': 2e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 190/1350 [5:31:55<34:51:11, 108.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6248, 'learning_rate': 2e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 191/1350 [5:33:49<35:19:48, 109.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9563, 'learning_rate': 2e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 192/1350 [5:35:46<36:04:17, 112.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6674, 'learning_rate': 2e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 193/1350 [5:37:40<36:08:16, 112.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.206, 'learning_rate': 2e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 194/1350 [5:39:40<36:53:27, 114.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 20.2412, 'learning_rate': 2e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 195/1350 [5:41:47<37:58:23, 118.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0317, 'learning_rate': 2e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 196/1350 [5:43:44<37:51:35, 118.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.3049, 'learning_rate': 2e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 197/1350 [5:45:24<36:02:53, 112.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4911, 'learning_rate': 2e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 198/1350 [5:46:58<34:12:59, 106.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6607, 'learning_rate': 2e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 199/1350 [5:48:40<33:47:20, 105.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7453, 'learning_rate': 2e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 200/1350 [5:50:46<35:41:02, 111.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2206, 'learning_rate': 2e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 201/1350 [5:53:00<37:44:08, 118.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6458, 'learning_rate': 2e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 202/1350 [5:54:56<37:29:55, 117.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.9193, 'learning_rate': 2e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 203/1350 [5:56:37<35:55:39, 112.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0215, 'learning_rate': 2e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 204/1350 [5:58:16<34:31:38, 108.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5205, 'learning_rate': 2e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 205/1350 [6:00:08<34:50:34, 109.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.056, 'learning_rate': 2e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 206/1350 [6:01:59<34:57:18, 110.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2313, 'learning_rate': 2e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 207/1350 [6:03:54<35:26:05, 111.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5004, 'learning_rate': 2e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 208/1350 [6:05:30<33:54:49, 106.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1948, 'learning_rate': 2e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 209/1350 [6:07:27<34:50:15, 109.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.1914, 'learning_rate': 2e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 210/1350 [6:09:15<34:36:01, 109.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3014, 'learning_rate': 2e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 211/1350 [6:11:25<36:35:53, 115.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7004, 'learning_rate': 2e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 212/1350 [6:12:57<34:17:25, 108.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8831, 'learning_rate': 2e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 213/1350 [6:14:44<34:09:05, 108.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.3776, 'learning_rate': 2e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 214/1350 [6:16:29<33:49:35, 107.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0063, 'learning_rate': 2e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 215/1350 [6:17:52<31:28:53, 99.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6044, 'learning_rate': 2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 216/1350 [6:19:24<30:40:09, 97.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.092, 'learning_rate': 2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 217/1350 [6:21:13<31:46:44, 100.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.1763, 'learning_rate': 2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 218/1350 [6:23:05<32:48:06, 104.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.1584, 'learning_rate': 2e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 219/1350 [6:24:56<33:23:24, 106.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2827, 'learning_rate': 2e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 220/1350 [6:26:27<31:53:34, 101.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4617, 'learning_rate': 2e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 221/1350 [6:28:16<32:37:15, 104.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6885, 'learning_rate': 2e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 222/1350 [6:30:08<33:17:38, 106.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.1877, 'learning_rate': 2e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 223/1350 [6:31:38<31:43:29, 101.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4842, 'learning_rate': 2e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 224/1350 [6:33:41<33:44:27, 107.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.667, 'learning_rate': 2e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 225/1350 [6:34:58<30:48:16, 98.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1663, 'learning_rate': 2e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 226/1350 [6:37:04<33:21:04, 106.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.181, 'learning_rate': 2e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 227/1350 [6:38:52<33:27:07, 107.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.348, 'learning_rate': 2e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 228/1350 [6:41:03<35:41:10, 114.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0235, 'learning_rate': 2e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 229/1350 [6:43:02<36:01:10, 115.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1497, 'learning_rate': 2e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 230/1350 [6:44:51<35:21:49, 113.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6128, 'learning_rate': 2e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 231/1350 [6:46:30<34:00:58, 109.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.344, 'learning_rate': 2e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 232/1350 [6:48:38<35:38:13, 114.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2324, 'learning_rate': 2e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 233/1350 [6:50:15<33:59:46, 109.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1227, 'learning_rate': 2e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 234/1350 [6:51:48<32:27:15, 104.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0145, 'learning_rate': 2e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 235/1350 [6:53:12<30:31:03, 98.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9044, 'learning_rate': 2e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 236/1350 [6:54:41<29:31:58, 95.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4133, 'learning_rate': 2e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 237/1350 [6:56:28<30:36:05, 98.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1849, 'learning_rate': 2e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 238/1350 [6:58:20<31:48:11, 102.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8375, 'learning_rate': 2e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 239/1350 [7:00:10<32:23:41, 104.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5076, 'learning_rate': 2e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 240/1350 [7:02:03<33:06:28, 107.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9705, 'learning_rate': 2e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 241/1350 [7:03:38<31:56:23, 103.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5035, 'learning_rate': 2e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 242/1350 [7:05:32<32:50:14, 106.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.8033, 'learning_rate': 2e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 243/1350 [7:07:20<32:57:54, 107.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8569, 'learning_rate': 2e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 244/1350 [7:09:31<35:05:46, 114.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.5604, 'learning_rate': 2e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 245/1350 [7:11:38<36:13:48, 118.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5643, 'learning_rate': 2e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 246/1350 [7:13:26<35:18:22, 115.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8368, 'learning_rate': 2e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 247/1350 [7:14:56<32:59:03, 107.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4071, 'learning_rate': 2e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 248/1350 [7:16:55<33:56:32, 110.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2866, 'learning_rate': 2e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 249/1350 [7:18:47<34:04:03, 111.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6328, 'learning_rate': 2e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 250/1350 [7:20:21<32:28:03, 106.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1433, 'learning_rate': 2e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 251/1350 [7:22:06<32:17:43, 105.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.003, 'learning_rate': 2e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 252/1350 [7:24:11<34:00:21, 111.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8143, 'learning_rate': 2e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 253/1350 [7:25:52<33:00:47, 108.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8592, 'learning_rate': 2e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 254/1350 [7:27:49<33:47:58, 111.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9548, 'learning_rate': 2e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 255/1350 [7:29:13<31:17:53, 102.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9571, 'learning_rate': 2e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 256/1350 [7:31:04<32:01:09, 105.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8088, 'learning_rate': 2e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 257/1350 [7:32:56<32:35:15, 107.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.113, 'learning_rate': 2e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 258/1350 [7:34:59<33:59:15, 112.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5466, 'learning_rate': 2e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 259/1350 [7:36:29<31:53:38, 105.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6464, 'learning_rate': 2e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 260/1350 [7:38:17<32:07:26, 106.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6136, 'learning_rate': 2e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 261/1350 [7:39:47<30:39:03, 101.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.3018, 'learning_rate': 2e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 262/1350 [7:41:40<31:40:42, 104.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.937, 'learning_rate': 2e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 263/1350 [7:43:33<32:22:24, 107.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2745, 'learning_rate': 2e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 264/1350 [7:45:21<32:25:32, 107.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8782, 'learning_rate': 2e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 265/1350 [7:47:03<31:55:05, 105.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9893, 'learning_rate': 2e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 266/1350 [7:48:48<31:46:13, 105.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1462, 'learning_rate': 2e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 267/1350 [7:50:50<33:17:27, 110.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9512, 'learning_rate': 2e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 268/1350 [7:52:51<34:08:03, 113.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.1487, 'learning_rate': 2e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 269/1350 [7:54:46<34:17:22, 114.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0485, 'learning_rate': 2e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 270/1350 [7:56:16<32:01:04, 106.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.514, 'learning_rate': 2e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 271/1350 [7:58:00<31:44:46, 105.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9838, 'learning_rate': 2e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 272/1350 [7:59:44<31:37:12, 105.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.738, 'learning_rate': 2e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 273/1350 [8:01:19<30:35:02, 102.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7357, 'learning_rate': 2e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 274/1350 [8:03:08<31:11:18, 104.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9018, 'learning_rate': 2e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 275/1350 [8:05:03<32:06:10, 107.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7215, 'learning_rate': 2e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 276/1350 [8:06:51<32:04:36, 107.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5758, 'learning_rate': 2e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 277/1350 [8:07:55<28:10:21, 94.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3817, 'learning_rate': 2e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 278/1350 [8:09:58<30:44:04, 103.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8759, 'learning_rate': 2e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 279/1350 [8:11:42<30:43:14, 103.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.876, 'learning_rate': 2e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 280/1350 [8:13:40<32:00:32, 107.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.3049, 'learning_rate': 2e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 281/1350 [8:15:38<32:56:42, 110.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4565, 'learning_rate': 2e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 282/1350 [8:17:00<30:20:33, 102.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4033, 'learning_rate': 2e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 283/1350 [8:18:52<31:10:15, 105.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8092, 'learning_rate': 2e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 284/1350 [8:20:29<30:26:10, 102.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8303, 'learning_rate': 2e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 285/1350 [8:22:10<30:14:11, 102.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7079, 'learning_rate': 2e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 286/1350 [8:24:01<30:59:58, 104.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0868, 'learning_rate': 2e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 287/1350 [8:25:27<29:16:04, 99.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8038, 'learning_rate': 2e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 288/1350 [8:27:30<31:19:24, 106.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5313, 'learning_rate': 2e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 289/1350 [8:29:17<31:23:31, 106.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9475, 'learning_rate': 2e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 290/1350 [8:31:01<31:06:01, 105.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4329, 'learning_rate': 2e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 291/1350 [8:33:24<34:24:16, 116.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 23.2452, 'learning_rate': 2e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 292/1350 [8:35:07<33:10:06, 112.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2069, 'learning_rate': 2e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 293/1350 [8:36:36<31:02:45, 105.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9629, 'learning_rate': 2e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 294/1350 [8:38:35<32:08:59, 109.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.7838, 'learning_rate': 2e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 295/1350 [8:40:45<33:56:15, 115.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6913, 'learning_rate': 2e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 296/1350 [8:42:27<32:38:37, 111.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3988, 'learning_rate': 2e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 297/1350 [8:44:22<32:57:49, 112.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0556, 'learning_rate': 2e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 298/1350 [8:46:31<34:20:04, 117.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4268, 'learning_rate': 2e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 299/1350 [8:48:15<33:06:26, 113.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8154, 'learning_rate': 2e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 300/1350 [8:50:14<33:33:27, 115.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4724, 'learning_rate': 2e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 301/1350 [8:52:05<33:11:30, 113.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5689, 'learning_rate': 2e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 302/1350 [8:54:00<33:14:53, 114.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4528, 'learning_rate': 2e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 303/1350 [8:55:31<31:11:53, 107.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9958, 'learning_rate': 2e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5d91e48ec5ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1892\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir=training_args.output_dir)\n",
    "\n",
    "print(\"Learnt model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53ed26b5-879e-4a22-b369-c7c43dd5b172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:26:43.212208Z",
     "iopub.status.busy": "2024-05-10T13:26:43.211768Z",
     "iopub.status.idle": "2024-05-10T13:26:43.488105Z",
     "shell.execute_reply": "2024-05-10T13:26:43.487253Z",
     "shell.execute_reply.started": "2024-05-10T13:26:43.212187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 71233fb6-958b-443d-a81e-b6eaef1836e0)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "399bd199-66bd-4b7c-82e2-84772bae48d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:27:24.760189Z",
     "iopub.status.busy": "2024-05-10T13:27:24.759567Z",
     "iopub.status.idle": "2024-05-10T13:27:24.773769Z",
     "shell.execute_reply": "2024-05-10T13:27:24.773202Z",
     "shell.execute_reply.started": "2024-05-10T13:27:24.760164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama_replaced_attn_64k_without_checkpoints_output_dir'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01993433-17b1-4866-8d07-b4b7e6850bb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:27:41.912894Z",
     "iopub.status.busy": "2024-05-10T13:27:41.912459Z",
     "iopub.status.idle": "2024-05-10T13:27:41.946651Z",
     "shell.execute_reply": "2024-05-10T13:27:41.946090Z",
     "shell.execute_reply.started": "2024-05-10T13:27:41.912873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47ae7848-0d9f-4b52-b95c-0163ea2f16f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:32:47.016331Z",
     "iopub.status.busy": "2024-05-10T13:32:47.015849Z",
     "iopub.status.idle": "2024-05-10T13:32:58.579268Z",
     "shell.execute_reply": "2024-05-10T13:32:58.578266Z",
     "shell.execute_reply.started": "2024-05-10T13:32:47.016303Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f18eec160d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' python3 passkey_retrivial.py          --context_size 64000          --base_model llama_replaced_attn_64k_without_checkpoints_output_dir          --max_tokens 32768          --interval 1000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/ml_kernel/kernel.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScriptExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_output_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_message_handlers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/ml_kernel/script_executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, lang, code)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mcode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process exited with code %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/ml_kernel/script_executor.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# push captured streams to intended destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.10/site-packages/IPython/utils/_process_common.py\u001b[0m in \u001b[0;36mget_output_error_code\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \"\"\"\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mout_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "! python3 passkey_retrivial.py \\\n",
    "        --context_size 64000 \\\n",
    "        --base_model llama_replaced_attn_64k_without_checkpoints_output_dir \\\n",
    "        --max_tokens 32768 \\\n",
    "        --interval 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21edd6d6-5a59-46ce-af85-e98b89d48272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:37:11.341560Z",
     "iopub.status.busy": "2024-05-10T12:37:11.341186Z",
     "iopub.status.idle": "2024-05-10T12:37:11.360953Z",
     "shell.execute_reply": "2024-05-10T12:37:11.360329Z",
     "shell.execute_reply.started": "2024-05-10T12:37:11.341540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32001, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32514a26-7fff-4f9b-bcd3-d899ff0b0151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:37:33.498618Z",
     "iopub.status.busy": "2024-05-10T12:37:33.498232Z",
     "iopub.status.idle": "2024-05-10T12:37:57.724336Z",
     "shell.execute_reply": "2024-05-10T12:37:57.723592Z",
     "shell.execute_reply.started": "2024-05-10T12:37:33.498597Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /tmp/xdg_cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "\n",
      "adapter_model.bin:   0%|          | 0.00/541M [00:00<?, ?B/s]\u001b[A\n",
      "adapter_model.bin:   0%|          | 8.19k/541M [00:00<3:23:48, 44.3kB/s]\u001b[A\n",
      "adapter_model.bin:   0%|          | 328k/541M [00:00<06:28, 1.39MB/s]   \u001b[A\n",
      "adapter_model.bin:   0%|          | 2.19M/541M [00:00<01:08, 7.88MB/s]\u001b[A\n",
      "adapter_model.bin:   1%|          | 5.27M/541M [00:00<00:37, 14.1MB/s]\u001b[A\n",
      "adapter_model.bin:   1%|▏         | 7.13M/541M [00:00<00:48, 11.0MB/s]\u001b[A\n",
      "adapter_model.bin:   2%|▏         | 9.11M/541M [00:00<00:40, 13.0MB/s]\u001b[A\n",
      "adapter_model.bin:   3%|▎         | 13.9M/541M [00:00<00:24, 21.7MB/s]\u001b[A\n",
      "adapter_model.bin:   3%|▎         | 16.4M/541M [00:01<00:41, 12.6MB/s]\u001b[A\n",
      "adapter_model.bin:   4%|▍         | 24.0M/541M [00:01<00:24, 21.3MB/s]\u001b[A\n",
      "adapter_model.bin:   5%|▌         | 27.1M/541M [00:01<00:34, 14.8MB/s]\u001b[A\n",
      "adapter_model.bin:   5%|▌         | 29.2M/541M [00:02<00:32, 15.7MB/s]\u001b[A\n",
      "adapter_model.bin:   6%|▌         | 32.0M/541M [00:02<00:28, 17.6MB/s]\u001b[A\n",
      "adapter_model.bin:   6%|▋         | 34.3M/541M [00:02<00:43, 11.7MB/s]\u001b[A\n",
      "adapter_model.bin:   7%|▋         | 37.8M/541M [00:02<00:33, 14.9MB/s]\u001b[A\n",
      "adapter_model.bin:   8%|▊         | 43.5M/541M [00:03<00:35, 13.9MB/s]\u001b[A\n",
      "adapter_model.bin:   8%|▊         | 45.4M/541M [00:03<00:34, 14.5MB/s]\u001b[A\n",
      "adapter_model.bin:   9%|▉         | 48.0M/541M [00:03<00:48, 10.3MB/s]\u001b[A\n",
      "adapter_model.bin:  10%|█         | 56.1M/541M [00:03<00:25, 18.9MB/s]\u001b[A\n",
      "adapter_model.bin:  11%|█         | 59.5M/541M [00:04<00:36, 13.3MB/s]\u001b[A\n",
      "adapter_model.bin:  11%|█▏        | 62.1M/541M [00:04<00:32, 14.7MB/s]\u001b[A\n",
      "adapter_model.bin:  12%|█▏        | 64.6M/541M [00:04<00:44, 10.7MB/s]\u001b[A\n",
      "adapter_model.bin:  14%|█▍        | 74.4M/541M [00:04<00:22, 21.2MB/s]\u001b[A\n",
      "adapter_model.bin:  15%|█▍        | 80.0M/541M [00:05<00:26, 17.6MB/s]\u001b[A\n",
      "adapter_model.bin:  17%|█▋        | 91.8M/541M [00:05<00:15, 29.5MB/s]\u001b[A\n",
      "adapter_model.bin:  18%|█▊        | 97.2M/541M [00:05<00:18, 23.7MB/s]\u001b[A\n",
      "adapter_model.bin:  20%|█▉        | 108M/541M [00:05<00:12, 34.4MB/s] \u001b[A\n",
      "adapter_model.bin:  21%|██        | 114M/541M [00:06<00:14, 29.3MB/s]\u001b[A\n",
      "adapter_model.bin:  23%|██▎       | 123M/541M [00:06<00:11, 37.3MB/s]\u001b[A\n",
      "adapter_model.bin:  24%|██▎       | 128M/541M [00:06<00:13, 29.6MB/s]\u001b[A\n",
      "adapter_model.bin:  26%|██▌       | 140M/541M [00:06<00:09, 42.0MB/s]\u001b[A\n",
      "adapter_model.bin:  27%|██▋       | 146M/541M [00:07<00:11, 35.4MB/s]\u001b[A\n",
      "adapter_model.bin:  29%|██▊       | 156M/541M [00:07<00:08, 44.1MB/s]\u001b[A\n",
      "adapter_model.bin:  30%|██▉       | 162M/541M [00:07<00:11, 32.3MB/s]\u001b[A\n",
      "adapter_model.bin:  31%|███       | 168M/541M [00:07<00:10, 36.9MB/s]\u001b[A\n",
      "adapter_model.bin:  32%|███▏      | 173M/541M [00:08<00:16, 21.7MB/s]\u001b[A\n",
      "adapter_model.bin:  33%|███▎      | 177M/541M [00:08<00:22, 16.2MB/s]\u001b[A\n",
      "adapter_model.bin:  34%|███▍      | 186M/541M [00:08<00:15, 23.1MB/s]\u001b[A\n",
      "adapter_model.bin:  35%|███▌      | 192M/541M [00:09<00:15, 22.2MB/s]\u001b[A\n",
      "adapter_model.bin:  38%|███▊      | 204M/541M [00:09<00:09, 34.0MB/s]\u001b[A\n",
      "adapter_model.bin:  39%|███▉      | 210M/541M [00:09<00:11, 28.5MB/s]\u001b[A\n",
      "adapter_model.bin:  41%|████      | 220M/541M [00:09<00:08, 37.9MB/s]\u001b[A\n",
      "adapter_model.bin:  42%|████▏     | 226M/541M [00:09<00:09, 31.7MB/s]\u001b[A\n",
      "adapter_model.bin:  44%|████▎     | 236M/541M [00:10<00:07, 41.8MB/s]\u001b[A\n",
      "adapter_model.bin:  45%|████▍     | 242M/541M [00:10<00:08, 33.3MB/s]\u001b[A\n",
      "adapter_model.bin:  46%|████▋     | 250M/541M [00:10<00:07, 41.4MB/s]\u001b[A\n",
      "adapter_model.bin:  47%|████▋     | 256M/541M [00:10<00:09, 29.9MB/s]\u001b[A\n",
      "adapter_model.bin:  49%|████▉     | 264M/541M [00:10<00:08, 34.1MB/s]\u001b[A\n",
      "adapter_model.bin:  50%|████▉     | 269M/541M [00:11<00:08, 32.7MB/s]\u001b[A\n",
      "adapter_model.bin:  50%|█████     | 273M/541M [00:11<00:15, 17.8MB/s]\u001b[A\n",
      "adapter_model.bin:  52%|█████▏    | 280M/541M [00:11<00:11, 23.6MB/s]\u001b[A\n",
      "adapter_model.bin:  53%|█████▎    | 284M/541M [00:12<00:14, 17.5MB/s]\u001b[A\n",
      "adapter_model.bin:  53%|█████▎    | 287M/541M [00:12<00:14, 18.0MB/s]\u001b[A\n",
      "adapter_model.bin:  54%|█████▎    | 290M/541M [00:12<00:19, 12.6MB/s]\u001b[A\n",
      "adapter_model.bin:  55%|█████▌    | 299M/541M [00:13<00:11, 20.4MB/s]\u001b[A\n",
      "adapter_model.bin:  56%|█████▌    | 304M/541M [00:13<00:13, 18.1MB/s]\u001b[A\n",
      "adapter_model.bin:  58%|█████▊    | 315M/541M [00:13<00:07, 29.3MB/s]\u001b[A\n",
      "adapter_model.bin:  59%|█████▉    | 320M/541M [00:13<00:09, 23.6MB/s]\u001b[A\n",
      "adapter_model.bin:  61%|██████    | 330M/541M [00:14<00:06, 32.3MB/s]\u001b[A\n",
      "adapter_model.bin:  62%|██████▏   | 336M/541M [00:14<00:07, 27.0MB/s]\u001b[A\n",
      "adapter_model.bin:  64%|██████▍   | 348M/541M [00:14<00:04, 39.3MB/s]\u001b[A\n",
      "adapter_model.bin:  65%|██████▌   | 354M/541M [00:14<00:05, 31.3MB/s]\u001b[A\n",
      "adapter_model.bin:  67%|██████▋   | 363M/541M [00:14<00:04, 39.8MB/s]\u001b[A\n",
      "adapter_model.bin:  68%|██████▊   | 369M/541M [00:15<00:05, 30.1MB/s]\u001b[A\n",
      "adapter_model.bin:  70%|███████   | 380M/541M [00:15<00:04, 40.0MB/s]\u001b[A\n",
      "adapter_model.bin:  71%|███████▏  | 386M/541M [00:15<00:05, 28.3MB/s]\u001b[A\n",
      "adapter_model.bin:  73%|███████▎  | 395M/541M [00:15<00:03, 36.6MB/s]\u001b[A\n",
      "adapter_model.bin:  74%|███████▍  | 401M/541M [00:16<00:04, 30.5MB/s]\u001b[A\n",
      "adapter_model.bin:  76%|███████▌  | 411M/541M [00:16<00:03, 40.9MB/s]\u001b[A\n",
      "adapter_model.bin:  77%|███████▋  | 418M/541M [00:16<00:03, 33.8MB/s]\u001b[A\n",
      "adapter_model.bin:  79%|███████▉  | 426M/541M [00:16<00:02, 42.1MB/s]\u001b[A\n",
      "adapter_model.bin:  80%|███████▉  | 432M/541M [00:17<00:03, 33.6MB/s]\u001b[A\n",
      "adapter_model.bin:  82%|████████▏ | 443M/541M [00:17<00:02, 45.4MB/s]\u001b[A\n",
      "adapter_model.bin:  83%|████████▎ | 450M/541M [00:17<00:02, 38.0MB/s]\u001b[A\n",
      "adapter_model.bin:  85%|████████▍ | 459M/541M [00:17<00:01, 46.5MB/s]\u001b[A\n",
      "adapter_model.bin:  86%|████████▌ | 465M/541M [00:17<00:02, 32.8MB/s]\u001b[A\n",
      "adapter_model.bin:  88%|████████▊ | 474M/541M [00:17<00:01, 40.8MB/s]\u001b[A\n",
      "adapter_model.bin:  89%|████████▊ | 480M/541M [00:18<00:01, 37.3MB/s]\u001b[A\n",
      "adapter_model.bin:  91%|█████████ | 491M/541M [00:18<00:01, 49.1MB/s]\u001b[A\n",
      "adapter_model.bin:  92%|█████████▏| 497M/541M [00:18<00:01, 34.4MB/s]\u001b[A\n",
      "adapter_model.bin:  94%|█████████▎| 506M/541M [00:18<00:00, 42.9MB/s]\u001b[A\n",
      "adapter_model.bin:  95%|█████████▍| 513M/541M [00:19<00:00, 30.3MB/s]\u001b[A\n",
      "adapter_model.bin:  97%|█████████▋| 523M/541M [00:19<00:00, 40.2MB/s]\u001b[A\n",
      "adapter_model.bin:  98%|█████████▊| 529M/541M [00:19<00:00, 23.8MB/s]\u001b[A\n",
      "adapter_model.bin: 100%|██████████| 541M/541M [00:20<00:00, 26.7MB/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/nvdenisov2002/llama-longLoRA-v4-64k-2160-samples-300-iterations/commit/d16cbcea47e6c79c3a0bad0b4d08ce68cd5d10f5', commit_message='Upload model', commit_description='', oid='d16cbcea47e6c79c3a0bad0b4d08ce68cd5d10f5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")\n",
    "model_id = \"nvdenisov2002/llama-longLoRA-v4-64k-2160-samples-300-iterations\"\n",
    "model.push_to_hub(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d162e8a-e0b3-469b-ae5e-d63cf237dca0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Some shit with generating by learnt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6048f965-540a-49a5-96bf-8945b26d61a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:36:38.736511Z",
     "iopub.status.busy": "2024-05-10T13:36:38.735912Z",
     "iopub.status.idle": "2024-05-10T13:36:38.754624Z",
     "shell.execute_reply": "2024-05-10T13:36:38.753722Z",
     "shell.execute_reply.started": "2024-05-10T13:36:38.736489Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prefix_len_and_tokens(tokenizer, row, diploma_prefix_len):\n",
    "    prompt_input_diploma = PROMPT_DICT[\"prompt_input_diploma_special\"]\n",
    "    source = prompt_input_diploma.format(input=row[\"diploma\"][:diploma_prefix_len])\n",
    "\n",
    "    # TODO: change for 16k new tokenizer\n",
    "    target = f\"{row['abstract']}{tokenizer.eos_token}\"\n",
    "\n",
    "    data_dict = preprocess([source], [target], tokenizer)\n",
    "    \n",
    "    prefix_len = np.sum(np.array(data_dict[\"labels\"][0]) == IGNORE_INDEX)\n",
    "    prefix_tokens = data_dict[\"input_ids\"][0][:prefix_len]\n",
    "\n",
    "    return prefix_len, prefix_tokens\n",
    "\n",
    "\n",
    "def get_some_model_result(some_model, tokenizer, row, device, diploma_prefix_len):\n",
    "    prefix_len, prefix_tokens = get_prefix_len_and_tokens(tokenizer, row, diploma_prefix_len)\n",
    "    print(prefix_len, prefix_tokens)\n",
    "    some_model.eval()\n",
    "    generated = some_model.generate(input_ids=prefix_tokens.reshape((1, -1)).to(device), do_sample=False, num_beams=1)\n",
    "    generated_continue = tokenizer.decode(generated.to('cpu').flatten()[prefix_len:])\n",
    "    return generated_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78467c77-ca94-4a43-b3c9-71b91e3211d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:51:51.529590Z",
     "iopub.status.busy": "2024-05-10T12:51:51.528980Z",
     "iopub.status.idle": "2024-05-10T12:51:51.542914Z",
     "shell.execute_reply": "2024-05-10T12:51:51.542245Z",
     "shell.execute_reply.started": "2024-05-10T12:51:51.529567Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41b6127d-2252-419a-82bc-c4fc46021fec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:54:59.375326Z",
     "iopub.status.busy": "2024-05-10T12:54:59.374640Z",
     "iopub.status.idle": "2024-05-10T12:54:59.391120Z",
     "shell.execute_reply": "2024-05-10T12:54:59.390443Z",
     "shell.execute_reply.started": "2024-05-10T12:54:59.375305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfd5cb89-270a-48be-ada7-5945505def4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:01:17.129681Z",
     "iopub.status.busy": "2024-05-10T13:01:17.129250Z",
     "iopub.status.idle": "2024-05-10T13:01:17.148682Z",
     "shell.execute_reply": "2024-05-10T13:01:17.147972Z",
     "shell.execute_reply.started": "2024-05-10T13:01:17.129661Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix_len, prefix_tokens = get_prefix_len_and_tokens(tokenizer, row, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1aece1b-1a2b-48a0-869d-546def144d2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:01:17.976857Z",
     "iopub.status.busy": "2024-05-10T13:01:17.976352Z",
     "iopub.status.idle": "2024-05-10T13:01:17.994806Z",
     "shell.execute_reply": "2024-05-10T13:01:17.994159Z",
     "shell.execute_reply.started": "2024-05-10T13:01:17.976836Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "         3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
       "        14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "        29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 21140,\n",
       "          340,   338,   263,   652,   572,  4125,  1426, 29889,  3575,  3414,\n",
       "          338,   304,  5706,  9846,   310,   445,   652,   572,  4125, 29889,\n",
       "           13,    13,  2277, 29937, 10567, 29901,    13, 30012, 29982,  2968,\n",
       "         6231,  3162,  1186,  8374, 19944,  9610,  3162, 19568,   733,  3840,\n",
       "        22807,  9860,  5460,  4470,  7956, 28287, 29901, 17442,  1755,   662,\n",
       "         8279,  2430,  3920, 16821, 10148,   490,   365, 26369, 29889,   939,\n",
       "        24643,  6457,   730,  7421, 23675, 11414,   933,   662,  8279,  3029,\n",
       "         3920, 16821, 14860,   665,  1695,  1488, 16352, 23567, 10148,  4355,\n",
       "        29964,  7792,   516, 26551, 30031, 30040, 30053,  1077, 29871, 29906,\n",
       "        29900, 29896, 29953, 29899, 29906, 29900, 29906, 29941, 18566,    13,\n",
       "           13,  2277, 29937, 13291, 29901])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97eec8d3-5bc2-45cb-8778-9326abfeacd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:48:21.779216Z",
     "iopub.status.busy": "2024-05-10T13:48:21.778549Z",
     "iopub.status.idle": "2024-05-10T13:48:21.794104Z",
     "shell.execute_reply": "2024-05-10T13:48:21.793420Z",
     "shell.execute_reply.started": "2024-05-10T13:48:21.779191Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,  263, 6635, 3290,  373,  263], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_input_ids = tokenizer(\"a cat sat on a\")[\"input_ids\"]\n",
    "some_input_ids = torch.tensor(some_input_ids, dtype=int).to('cuda')\n",
    "some_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f171142e-74b0-4ffe-884a-18941c34bbea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:48:22.460702Z",
     "iopub.status.busy": "2024-05-10T13:48:22.460215Z",
     "iopub.status.idle": "2024-05-10T13:48:22.590773Z",
     "shell.execute_reply": "2024-05-10T13:48:22.589639Z",
     "shell.execute_reply.started": "2024-05-10T13:48:22.460669Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 32, 1, 128] at entry 0 and [1, 32, 7, 128] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-55c7aad54168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msome_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2735\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 )\n\u001b[1;32m    924\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    926\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    636\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py\u001b[0m in \u001b[0;36mforward_flashattn\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# transform the data into the format required by flash attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     qkv = torch.stack(\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     )  # [bsz, nh, 3, q_len, hd]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 32, 1, 128] at entry 0 and [1, 32, 7, 128] at entry 1"
     ]
    }
   ],
   "source": [
    "generated = model.generate(input_ids=some_input_ids.reshape((1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a67e42a9-3376-41d7-8254-e23b0f3f14d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:37:35.968100Z",
     "iopub.status.busy": "2024-05-10T13:37:35.967690Z",
     "iopub.status.idle": "2024-05-10T13:37:36.084943Z",
     "shell.execute_reply": "2024-05-10T13:37:36.083865Z",
     "shell.execute_reply.started": "2024-05-10T13:37:35.968071Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 32, 1, 128] at entry 0 and [1, 32, 146, 128] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-2dbc6444d1eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2735\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 )\n\u001b[1;32m    924\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    926\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    636\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py\u001b[0m in \u001b[0;36mforward_flashattn\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# transform the data into the format required by flash attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     qkv = torch.stack(\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     )  # [bsz, nh, 3, q_len, hd]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 32, 1, 128] at entry 0 and [1, 32, 146, 128] at entry 1"
     ]
    }
   ],
   "source": [
    "generated = model.generate(input_ids=prefix_tokens.reshape((1, -1)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7f00960-450a-4ce4-8858-3636cebddbe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:58:16.543433Z",
     "iopub.status.busy": "2024-05-10T12:58:16.543037Z",
     "iopub.status.idle": "2024-05-10T12:58:16.678409Z",
     "shell.execute_reply": "2024-05-10T12:58:16.677308Z",
     "shell.execute_reply.started": "2024-05-10T12:58:16.543414Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 32, 1, 128] at entry 0 and [1, 32, 146, 128] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cea616c1a103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1607\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2455\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 )\n\u001b[1;32m    924\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    926\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    636\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py\u001b[0m in \u001b[0;36mforward_flashattn\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# transform the data into the format required by flash attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     qkv = torch.stack(\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     )  # [bsz, nh, 3, q_len, hd]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 32, 1, 128] at entry 0 and [1, 32, 146, 128] at entry 1"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "generated = model.generate(input_ids=prefix_tokens.reshape((1, -1)).to(device), do_sample=False, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6441b4e4-437f-40d4-aaa9-ae3ea682af7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:55:15.086393Z",
     "iopub.status.busy": "2024-05-10T12:55:15.085972Z",
     "iopub.status.idle": "2024-05-10T12:55:15.244797Z",
     "shell.execute_reply": "2024-05-10T12:55:15.243349Z",
     "shell.execute_reply.started": "2024-05-10T12:55:15.086375Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
      "         3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
      "        14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
      "        29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 21140,\n",
      "          340,   338,   263,   652,   572,  4125,  1426, 29889,  3575,  3414,\n",
      "          338,   304,  5706,  9846,   310,   445,   652,   572,  4125, 29889,\n",
      "           13,    13,  2277, 29937, 10567, 29901,    13, 30012, 29982,  2968,\n",
      "         6231,  3162,  1186,  8374, 19944,  9610,  3162, 19568,   733,  3840,\n",
      "        22807,  9860,  5460,  4470,  7956, 28287, 29901, 17442,  1755,   662,\n",
      "         8279,  2430,  3920, 16821, 10148,   490,   365, 26369, 29889,   939,\n",
      "        24643,  6457,   730,  7421, 23675, 11414,   933,   662,  8279,  3029,\n",
      "         3920, 16821, 14860,   665,  1695,  1488, 16352, 23567, 10148,  4355,\n",
      "        29964,  7792,   516, 26551, 30031, 30040, 30053,  1077, 29871, 29906,\n",
      "        29900, 29896, 29953, 29899, 29906, 29900, 29906, 29941, 18566,    13,\n",
      "           13,  2277, 29937, 13291, 29901])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n",
      "/home/jupyter/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py:39: UserWarning: This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\n",
      "  warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 32, 1, 128] at entry 0 and [1, 32, 146, 128] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-eaf7152043d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Это диплом о том, как писать диплом автоматически с помощью LLM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"diploma\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Выпускная квалификационная работа по Компьютерным наукам: использование длинных контекстов в LLM. В этой работе рассмотрены длинные контексты на примерах текстов дипломов СПБГУ за 2016-2023 годы\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_some_model_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-79e6fa33a87b>\u001b[0m in \u001b[0;36mget_some_model_result\u001b[0;34m(some_model, tokenizer, row, device, diploma_prefix_len)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msome_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msome_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mgenerated_continue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated_continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1607\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2455\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 )\n\u001b[1;32m    924\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    926\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    636\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/LongLoRA-diploma-research/llama_attn_replace_sft.py\u001b[0m in \u001b[0;36mforward_flashattn\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# transform the data into the format required by flash attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     qkv = torch.stack(\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     )  # [bsz, nh, 3, q_len, hd]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 32, 1, 128] at entry 0 and [1, 32, 146, 128] at entry 1"
     ]
    }
   ],
   "source": [
    "row = {\"abstract\": \"Это диплом о том, как писать диплом автоматически с помощью LLM\", \"diploma\": \"Выпускная квалификационная работа по Компьютерным наукам: использование длинных контекстов в LLM. В этой работе рассмотрены длинные контексты на примерах текстов дипломов СПБГУ за 2016-2023 годы\"}\n",
    "get_some_model_result(model, tokenizer, row, device, INF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8b16a8d-376d-4077-9fd6-e1d1a3745965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:46:42.853402Z",
     "iopub.status.busy": "2024-05-10T12:46:42.852974Z",
     "iopub.status.idle": "2024-05-10T12:46:42.900792Z",
     "shell.execute_reply": "2024-05-10T12:46:42.900082Z",
     "shell.execute_reply.started": "2024-05-10T12:46:42.853381Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nBelow is a diploma text. Your task is to generate abstract of this diploma.\\n\\n### Input:\\nФедеральное государственное бюджетное  образовательное учреждение высшего образования «Санкт-Петербургский государственный университет»\\n\\n\\n\\n\\nВЫПУСКНАЯ КВАЛИФИКАЦИОННАЯ РАБОТА\\n\\nНА ТЕМУ: СТРУКТУРА ПАРОДОНТОЛОГИЧЕСКИХ ЗАБОЛЕВАНИЙ У ПАЦИЕНТОВ РАЗНЫХ ВОЗРАСТНЫХ ГРУПП.\\n\\n\\n\\n\\n\\n\\nВыполнила студентка \\nЖивилова Ксения Александровна\\n526 группы\\nНаучный руководитель\\nК.м.н. Шевелева Наталья Александровна\\n\\n\\n\\n\\n\\n\\n\\nСанкт-Петербург\\n2018\\x0c\\nОГЛАВЛЕНИЕ \\nПеречень условных обозначений…………….………………………………..3\\nВведение……………………………………………..…….……………………...5\\nАктуальность …………………………………….……………..………………...5\\nЦель исследования……………………………………………………………….7\\nЗадачи исследования…………………………………………….........................7\\nПрактическая значимость исследования……………………….........................7\\nГлава 1. Литературный обзор………………………………………………...8\\n\\t•\\tПародонт……………………………………………………………………..8\\n1.2 Этиология и патогенез воспалительных заболеваний пародонта………...9\\n1.3 Классификация пародонтологических заболеваний………………………13\\n1.3.1 Гингивит……………………………………………………………………15\\n1.3.2 Пародонтит…………………………………………………………………24\\n1.3.3 Пародонтоз…………………………………………………………………28\\nГлава 2. Материалы и методы исследования………………………………31\\n2.1 Характеристика обследованных пациентов…………...…………………...31\\n2.2 Оценка стоматологического статуса пациентов…………………………...32\\n2.3 Оценка рентгеновских снимков ……………………………………………36\\n2.4 Параклинические методы…………………………………………………...37\\nГлава 3. Результаты исследований…………………………………………..38\\n3.1 Результаты клинического исследования…………………………………...38\\n3.2 Результаты рентгенологического исследования…………………………..50\\nЗаключение и выводы\\nЗаключение……………………………………………………………………….52\\nВыводы……..…………………………………………………………………….53\\nПрактические рекомендации…………………………………….……………..54\\nСписок использованной литературы………………………….…………….55\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nПЕРЕЧЕНЬ УСЛОВНЫХ ОБОЗНАЧЕНИЙ \\nВОЗ – всемирная организация здравоохранения\\nВЗП – воспалительное заболевание пародонта\\nХГП – хронический генерализованный пародонтит\\nХБП – хроническая болезнь почек\\nИГ – индекс гигиены\\nCPITN - Community Periodontal Index of Treatment Needs\\nPMA - папиллярно–маргинально–альвеолярный индекс \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nВВЕДЕНИЕ\\nАктуальность:\\nЗдоровье полости рта, как считают специалисты, представляет собой одну из главных основ общего здоровья человека. Официальная статистика ВОЗ гласит, что примерно 98% людей во всем мире страдают воспалительными заболеваниями тканей пародонта. (Матвеев А.М., 2013)\\nРаспространенность заболеваний пародонта у лиц среднего и пожилого возраста по данным ВОЗ занимают первое место среди всех стоматологических заболеваний.\\nПо данным ВОЗ (2002), около 95% взрослого населения планеты и 80% детского населения имеют те или иные признаки пародонтопатий. Высокий уровень заболеваний пародонта, по докладу научной группы ВОЗ, выпадает на возраст 20 – 44 года (от 65 – 95%) и 15 – 19 лет (от 55 – 89%).\\nРаспространенность заболеваний пародонта в России, в зависимости от возраста, колеблется от 48,2% (в 12 лет) до 86,2% (в 44 года), а к 60 – 65 годам достигает 100%. Наиболее часто встречающейся патологией пародонта в молодом возрасте является гингивит, после 30 лет – пародонтит.\\nМногочисленные исследования (Алимский А.В., 1989; 2000; Грудянов А.И., 1994; Барер Г.М., 1995; Борисова Е.Н., 2001; Иванов В.Ф., 1998, 2001; Beck J.D., Slade G.D., 1996; Gjermo P.E., 1998 и др.) показали, что лишь у 12% людей здоровый пародонт, у 53% отмечены начальные воспалительные изменения, у 23% – начальные деструктивные изменения, а у 12% имеются поражения средней и тяжелой степени: начальные воспалительные и деструктивные изменения очень часто (в 38% и 23% соответственно) встречаются у лиц в возрасте 25 – 34 лет, однако деструктивные изменения средней и тяжелой степеней у них встречаются более чем в 3 раза чаще, чем в предыдущей группе.\\nВ возрастных группах 35 – 44, 45 – 54, 55 лет и старше число лиц с начальными изменениями пародонта прогрессивно уменьшается 26 – 15% при одновременном росте изменений средней и тяжелой степени – до 75%.\\nОбращаемость населения с заболеваниями пародонта существенно возросла за последнее десятилетие и достигла 64% от общего амбулаторного приема [Курякина Н. В., 2007; Янушевич О. О., 2009; Леонова Л. Е., 2008]. По данным ВОЗ, заболевания пародонта тяжелой степени выявляются у 5-25% взрослого населения, средней степени — у 30-45% и только 2-8% людей имеют здоровые ткани пародонта в возрасте 35-45 лет. Показатели заболеваемости пародонтитом, одной из широко распространенных стоматологических патологий, на сегодняшний день имеют тенденцию к увеличению. [Меньшикова Ю. В., 2011; Грудянов А. И., 2006; Воронина А. И., 2010]\\nЗначимость ее определяется огромной распространенностью различных форм патологии пародонта, тяжестью течения некоторых из них и отрицательным влиянием на организм в целом, изменением качества жизни человека. \\nПоражая практически все возрастные группы населения, в том числе молодых людей и даже детей, заболевания пародонта способствуют формированию «нездоровой» нации. Именно они приводят чаще всего к потере зубов вследствие выраженного деструктивного процесса, обусловливая стойкие нарушения функции зубочелюстной системы [89,91,347]. \\n\\n\\n\\n\\n\\n\\n\\nЦель исследования: \\nПроведение аналитического исследования структуры пародонтологических заболеваний у пациентов разных возрастных групп.\\nЗадачи исследования:\\n\\t•\\tИзучить структуру заболеваний пародонта у пациентов разных возрастных групп.\\n\\t•\\tИзучить соматический статус пациентов разных возрастных групп.\\n\\t•\\tПровести анализ взаимосвязи соматических и пародонтологических заболеваний у пациентов разных возрастных групп.\\n\\t•\\tНа основе проведенного исследования изучить методы профилактики пародонтологических заболеваний у пациентов разных возрастных групп с учетом соматического статуса.\\nПрактическая значимость исследования:\\nВ проведенном нами исследовании была изучена структура пародонтологических заболеваний у пациентов разных возрастных групп. В ходе работы была выявлена взаимосвязь заболеваний пародонта и соматических заболеваний у пациентов разных возрастных групп, что может явиться основой для разработки наиболее эффективных методов профилактики и диспансеризации пациентов в зависимости от возраста, а также лечь в основу индивидуального подхода к лечению заболеваний пародонта у пациентов разных возрастных групп.\\n\\n\\n\\n\\n\\n\\n\\nГЛАВА 1. ЛИТЕРАТУРНЫЙ ОБЗОР\\nПародонт – это ткани, окружающие зуб, т.е. десна с надкостницей, соединительная ткань вокруг корня зуба, а также кость челюсти, в которой крепится зуб.\\nБольшинство заболеваний пародонта возникают вследствие воспалительного процесса, который вызывает разрушение десен, связочного аппарата и костной ткани. А «запускающим механизмом» воспаления являются патогенные микробы. Скапливаясь, они приводят к образованию микробного налета. С участием слюны он преобразуется в зубной камень, который разрастается между зубом и десной. Следовательно возникает зубодесневой карман. Чем сильнее выражен воспалительный процесс, тем серьезнее заболевание.\\nПричиной патологического процесса в тканях пародонта могут быть различные факторы как экзогенного, так и эндогенного происхождения. Несмотря на разнообразие этих факторов, воспалительный или дистрофически-воспалительный процесс в тканях пародонта протекает довольно однотипно и в зависимости от локализации, длительности воздействия этиологических факторов проявляется различными морфологическими и патофизиологическими вариантами. Состояние физиологических защитных механизмов тканей пародонта и организма в целом определяет степень распространенности дистрофически-воспалительного процесса и его интенсивность.\\nМЕСТНЫЕ ФАКТОРЫ\\nИз комплекса местных факторов, влияющих на состояние тканей пародонта, следует выделить зубные отложения, микрофлору, травматическую окклюзию, несанированную полость рта; неполноценные пломбы, протезы, ортодонтические аппараты; вредные привычки, неправильное расположение уздечек губ, языка и пр.\\n\\n\\nОбщие факторы \\nГиповитаминоз, иммунологические реакции организма, нервно-трофические нарушения, эндокринные нарушения, нарушение обмена веществ, сосудистые изменения\\nПо данным статистики, заболевания пародонта встречаются у 12-20% детей в возрасте 5-12 лет. Хронический пародонтит выявляют у 20-40% людей до 35 лет и у 80-90% населения в возрасте после 40 лет. Пародонтоз встречается в 4-10% случаев. Наиболее высокий показатель распространенности заболеваний пародонта наблюдается среди пациентов старших возрастных групп. [Барер Г.М. 2013; Данилевский Н.Ф., Борисенко А.В. 2000]\\n1.2 Этиология и патогенез воспалительных заболеваний пародонта\\nНа развитие заболеваний пародонта могут влиять местные факторы, такие как функциональная травма, микроорганизмы зубных отложений, а также сочетанное воздействие местных и общих факторов на фоне измененной реактивности организма. Микробная бляшка (зубной налет) является первопричиной хронических воспалительных процессов в пародонте. [Барер Г.М., 2013] \\nМестные факторы, способствующие развитию воспалительных заболеваний пародонта:\\n- состав и свойства слюны;\\n- травматическая окклюзия;\\n- отсутствие контактов между зубами;\\n- зубной камень;\\n- аномалии положения отдельных зубов и патология прикуса;\\n- поверхности некачественных пломб, коронок и протезов;\\n- особенности строения мягких тканей (прикрепление уздечек губ и мышечных тяжей). \\n\\n\\nСистемные факторы в патогенезе пародонтита:\\n- эндокринные нарушения;\\n- гипоавитаминозы и авитаминозы;\\n- атеросклеротическое поражение сосудов;\\n- хронические заболевания печени, толстой кишки, желудка;\\n-болезни крови и гемопоэтической системы;\\n-иммунологические нарушения;\\n- патология почек;\\n- стресс;\\n- курение табака;\\n- генетическая предрасположенность.\\nВсе вышеизложенные факторы влияют  на защитную систему пародонта и создают условия для патогенного действия микробиоты на ткани пародонта и прежде всего на зубодесневое прикрепление, воспаление и деструкция которого приводят к формированию пародонтального кармана и развитию пародонтита. [Орехова Л. Ю., 2004, Мюллер Х. П., 2004]\\nДействие патогенных факторов возможно только в том случае, если превышают защитные силы пародонта и при снижении реактивности организма. [Данилевский Н.Ф., Борисенко А.В., 2000]\\nОсновной концепцией развития пародонтита в ХХ веке служила «неспецифическая бляшечная» теория, предложенная Walter Loesche в 1976 году. Согласно этой теории, все бактерии бляшки в равной степени могут быть возбудителями заболевания при условии достижения количества микроорганизмов в бляшке определенного уровня, и организм уже не может противостоять патологическим процессам, провоцируемых воздействием токсических продуктов бактерий. Но данная точка зрения имеет несколько неразрешенных, например, что обилие мягкого зубного налета приводит к развитию гингивита, но далеко не у всех людей данное состояние перерастает в пародонтит, а если и прогрессирует, то непораженные участки пародонта соседствуют с пораженными несмотря на одинаковый патогенный потенциал зубной бляшки. \\n \\tВ середине 70-х годов прошлого века была сформулирована иная гипотеза, согласно которой не все бактерии способствуют деструкции тканей пародонта, а только те, которые обладают специфическим патогенным потенциалом для возникновения заболевания. Данная теория была названа «специфической бляшечной». Сторонники данной теории занимались вычислением этих особенных микроорганизмов и выясняли причины их колонизации.  В  ходе нескольких экспериментов было определено, что к возбудителям пародонтита относятся преимущественно грамотрицательные и анаэробные виды микроорганизмов.\\n\\tВозбудители, способствующие возникновению пародонтита:\\n\\t•\\tГрамотрицательные анаэробы – Porphyromonas gingivalis, Tannerella forsythia, Treponema denticola, Fusobacterium nucleatum, Compylobacter rectus, Prevotella intermedia и другие спирохеты полости рта;\\n\\t•\\tГрамотрицательные факультативные анаэробы – Eikenella corrodens, Actinobacillus actinomycetemcomitans;\\n\\t•\\tГрамположительные анаэробы – Peptostreptococcus micros, Eubacterium nodatum, Streptococcus intermedius.\\nРедко и в небольшом количестве они также могут обнаруживаться и у здоровых лиц. \\n\\tВ настоящее время в качестве альтернативы «специфической» теории была выдвинута «экологическая» теория, которая рассматривает влияние внешних факторов на экологию биопленки. То есть внешние факторы инициируют изменение состава нормальной микробиоты биопленки и это, в конечном счете, приводит к преобладанию патогенных видов микроорганизмов. Сторонниками этой теории акцентируется внимание на том, что отдельные микроорганизмы вряд ли сами по себе могут самостоятельно спровоцировать патологический воспалительный процесс, ведущий к деструкции пародонта. [Ричард Дж.Ламонт, Мэрилин С.Лантц, 2010]\\nБыло выявлено пять бактериальных пародонтопатогенных комплексов, бактерии которых связаны друг с другом в биопленке: «красный», «зеленый», «желтый», «пурпурный» и «оранжевый». (Socransky S.S., Haffajee A.A., 2000; Люговская А.В., Юдина Н.А., 2009)\\n\\t•\\t«Красный комплекс», обладающий наивысшим патогенным потенциалом,  обладают агрессивным воздействием на ткани пародонта, как результат сильная кровоточивость десен и быстрое течение деструктивных процессов в пародонте. В него входят – Porphyromonas gingivalis, Tannerella forsythia, Treponema denticola; \\n\\t•\\t«Оранжевый комплекс» – Prevotella intermedia, Prevotella nigrescens, Fusobacterium nucleatum, Fusobacterium polymorphum, Peptostreptococcus micros, Compylobacter rectus, и другие. Обнаруживается при быстропрогрессирующих формах болезней пародонта;\\n\\t•\\t«Желтый комплекс» –Streptococcus mitis, Streptococcus oralis, Streptococcus sanguis. Могут играть защитную роль, вступая в антагонистические взаимодействия с периодонтальными патогенами, но роль этих комплексов до конца не изучена.\\n\\t•\\t«Зеленый комплекс» является причиной как заболеваний пародонта, так и иных поражений слизистой оболочки рта и твердых тканей зубов. В него входят – Actinobacillus actinomycetemcomitans, Eikenella corrodens, Capnocytophaga ochracea, Capnocytophaga gingivalis, Capnocytophaga sputigena; \\n\\t•\\t«Пурпурный комплекс» - Veilonella parvula, Actinomyces odontolyticus.\\nИменно эти микроорганизмы чаще всего определяются при ВЗП, поэтому их и назвали пародонтопатогенными. [Socransky S.S., Haffajee A.A., 2000; Грудянов А. И., Овчинникова В. В., 2007]\\n\\tСреди перечисленных микроорганизмов основную пародонтопатогенную роль играют анаэробные микроорганизмы: Porphyromonas Gingivalis, Tannerella forsythia, Treponema denticola Prevotela intermedia и др. Доказано преобладание анаэробной микробиоты в пародонтальных карманах при обострении ХГП средней и тяжелой степеней. [Ximenez-Fyvie L.A. [et al.], 2000; Bodet C. [et al.], 2006; Дрижал И., 2001] Особенностью воздействия этих анаэробов является то, что они выделяют эндотоксины, повреждающие клетки и межклеточные структуры пародонта. Эти микроорганизмы обитают в местах, лишенных доступа кислорода: в обильных скоплениях зубного налета, зубного камня. Поэтому они идентифицируются в глубоких пародонтальных карманах при пародонтите средней и тяжелой степеней. Особенностью влияния пародонтопатогенов является выделение чрезвычайно активных эндотоксинов, повреждающие все ткани пародонта, включая кость; в начале развития заболевания процесс деструкции происходит медленно, а затем резко усиливается. [Грудянов А. И., Овчинникова В. В., 2007]\\n\\n1.3 КЛАССИФИКАЦИЯ ПАРОДОНТОЛОГИЧЕСКИХ ЗАБОЛЕВАНИЙ\\nГингивит\\xa0— это воспаление десен без нарушения целостности зубодесневого соединения. При отсутствии лечения гингивит может прогрессировать в деструктивную форму заболеваний пародонта\\xa0— пародонтит.\\nГингивит, как правило, возникает из-за скопления микробного налёта на зубах, в результате несоблюдения гигиены полости рта. Гингивит также может обусловить неправильное ортодонтическое лечение, которое в комплексе с плохим уходом за зубами и полостью рта провоцирует интенсивное развитие патогенных микроорганизмов. Бактерии (реже\\xa0— вирусы, грибы) являются непосредственной причиной воспаления дёсен (Streptococcus oralis, Bacteroides gingivalis, Porphyromonas gingivalis, Actinomycetes comitans, Prevotella intermedia, Actinomyces israelii)\\n\\nФормы\\nТяжесть\\nТечение\\nРаспространенность\\nКатаральный\\nЛегкая\\nОстрое\\nЛокализованный\\nЯзвенно-некротический\\nСредняя\\nХроническое\\nГенерализованный\\nГипертрофический\\nтяжелая\\nобострившееся\\n\\n\\nФакторы риска\\n\\t•\\tнарушение гигиены полости рта;\\n\\t•\\tиммунодепрессивные состояния;\\n\\t•\\tотсутствие доступа к адекватной стоматологической помощи;\\n\\t•\\tнедоедание;\\n\\t•\\tзубной камень;\\n\\t•\\tвозраст от 3 до 6 лет;\\n\\t•\\tсахарный диабет;\\n\\t•\\tбеременность;\\n\\t•\\tдефицит витамина C;\\n\\t•\\tдепрессии;\\n\\t•\\tОРЗ, грипп, ангина, СПИД, туберкулёз и другие заболевания;\\n\\t•\\tотравление организма тяжёлыми металлами (ртуть, висмут, свинец);\\n\\t•\\tприменение оральных контрацептивов;\\n\\t•\\tпатологии прикуса;\\n\\t•\\tпроблемные пломбы;\\n\\t•\\tнарушение носового дыхания.\\n[Данилевский Н.Ф; Борисенко А.В.]\\n\\n\\n\\n\\n\\n\\n1.3.1 Гингивит\\u2028Катаральный гингивит\\nКатаральный гингивит - заболевание пародонта, характеризующееся серозным (катаральным) воспалением десны, затрагивающее поверхностные ткани пародонта и протекающее без повреждения зубодесневого прикрепления.\\nМестные изменения при катаральном гингивите включают отек, гиперемию (или цианотичность) слизистой оболочки десны, болезненность и кровоточивость десневого края, наличие зубных отложений, неприятный привкус в полости рта.\\nКатаральный гингивит является самой распространённой формой заболевания.\\nПричины катарального гингивита\\nКатаральный гингивит возникает под воздействием местных и системных факторов. Катаральный гингивит у детей может быть связан с процессом прорезывания зубов; в этом случае после выхода из десны коронки зуба воспаление стихает.\\nЛокальными факторами, способствующими развитию катарального гингивита, могут служить травмы зуба (перелом коронки, вывих зуба и др.), пришеечный кариес, неправильный прикус, аномалии зубов (дистопия, скученность) и мягких тканей ротовой полости (короткая уздечка губ, мелкое преддверие); неудовлетворительный уход за зубами, зубной камень, дефекты постановки пломб, зубных протезов, эстетических виниров или ортодонтических аппаратов и т. д.\\nВ этиологии катарального гингивита большую роль играют общие факторы, обусловливающие повышенную подверженность некоторых людей воспалительным заболеваниям пародонта. В их числе такие физиологические периоды жизни, как пубертатный возраст, беременность, менопауза; вредные привычки (курение); заболевания (сахарный диабет, язвенная болезнь желудка, хронический гепатит, гипо- и гипертиреоз, лейкемия, ВИЧ-инфекция и др.); вирусные инфекции (грипп, ОРВИ); гипо- и авитаминозы (цинга, пеллагра); прием лекарственных препаратов (цитостатиков, иммунодепрессантов, оральных гормональных контрацептивов).\\nОбщепризнанным пусковым механизмом развития катарального гингивита считается наличие зубного налета (микробной бляшки, или биопленки). В состав микробной бляшки входят аэробные (стафилококки, стрептококки, актиномицеты) и анаэробные микроорганизмы (фузобактерии, превотеллы, порфиромонады, трепонемы и др.) с преобладанием последних. Повреждающий потенциал микробных скоплений во многом зависит от состояния защитных сил организма и иммунного статуса. Таким образом, основными провоцирующими моментами в развитии катарального гингивита выступают неудовлетворительная гигиена полости рта и нарушение общего гомеостаза организма.\\nВ зависимости от характера течения катаральный гингивит бывает острым и хроническим. По степени распространенности воспаления катаральный гингивит может иметь локализованную (в области 1-3-х зубов) или генерализованную, диффузную (в области одной или обеих челюстей) форму.\\nС учетом тяжести поражения в пародонтологии различают 3 степени катарального гингивита:\\n\\t•\\tлегкую – с воспалительным поражением зубодесневых сосочков. \\n\\t•\\tсреднюю – с воспалением межзубной и маргинальной части десны.\\n\\t•\\tтяжелую – с вовлечением в воспалительный процесс всей десны, включая ее альвеолярную часть. [Козлов В.А 2003; Барер Г.М 2008]\\u2028Симптомы катарального гингивита\\nПри остром катаральном гингивите определяется гиперемия, отечность десны в области нескольких или всех зубов. Характерна кровоточивость десен, выраженность которой зависит от интенсивности воспаления. Отмечается жжение и боль в пораженных участках. Болевые ощущения и кровоточивость слизистой десны усиливаются во время приема пищи, пальпации, чистки зубов, зондирования. За редким исключением, общее состояние при катаральном гингивите обычно не нарушается. При тяжелом течении катарального гингивита могут возникать гипертермия, мышечные боли, общее недомогание. \\nПри хронической форме катарального гингивита десна приобретает цианотичную окраску (застойная гиперемия), валикообразное утолщение. Кровоточивость возникает при малейшей травме. Отмечается ощущение распирания в десне, постоянный привкус крови, нередко - неприятный запах изо рта. В период обострения настоящие жалобы усиливаются.\\nПри осмотре выявляется изменение цвета и рельефа десны: она становится ярко-красной и рыхлой; десневой край теряет свою фестончатость; межзубные сосочки приобретают куполообразную форму; иногда определяются участки десквамации слизистой и единичные эрозии. Типично наличие повышенного содержания неминерализованного зубного налета или зубного камня. Патологические зубодесневые карманы при катаральном гингивите отсутствуют; зубы сохраняют устойчивость и неподвижность.\\n\\nЯЗВЕННО-НЕКРОТИЧЕСКИЙ ГИНГИВИТ\\nЯзвенно-некротический гингивит (гингивит Венсана) является инфекционно-воспалительным заболеванием десен, которое имеет тенденцию к самостоятельному развитию или возникновению на фоне общих заболеваний организма, осложняя течение патологического процесса. Воспаление при язвенном гингивите сопровождается некротическими изменениями и появлением язвенных дефектов на поверхности слизистой оболочки десны.\\nОстрый язвенный гингивит появляется при токсическом воздействии на организм различными вредными веществами (к примеру, тяжелыми металлами), при хронических ожогах химическими соединениями или интоксикации организма тяжелыми инфекционными заболеваниями (например, при лейкозах).\\nСущественную роль в возникновении гингивита Венсана играет образ жизни. При постоянном нервном напряжении и частых стрессах риск заболеть им увеличивается почти в два раза.\\nНачало болезни характеризуется проявлением симптомов в острой форме на фоне интоксикации всего организма или при местном воздействии токсического вещества на десны.\\nДля язвенного гингивита типично острое начало и бурное течение болезни. Ткани отекают, межклеточное пространство наполняется фиброзным экссудатом.\\nПоверхностный слой слизистой десны инфильтрируется лейкоцитами. Ткани подвергаются некротическим изменениям и появляются эрозии, быстро трансформирующиеся в язвы.\\nЗаживление язв и восстановление нормального состояния тканей при язвенно-некротическом гингивите протекает очень медленно. Если лечение было начато, когда заболевание уже находилось в запущенном состоянии, на слизистых покровах останутся рубцы от заживших язв и даже серьезные деформации тканей.\\nЕсли болезнь сопровождается пародонтитом, то зубодесневые связки подвергаются разрушению и обнажаются нижние отделы периодонта. Инфекция может легко туда проникнуть, так как и барьерная, и иммунная формы защиты почти уже не действуют. [Дедов И.И 2007; Барер Г.М 2008; Козлов В.А 2003]\\nПричины гингивита Венсана\\nЯзвы возникают на фоне воспаленной десны и сопровождаются некротическими и язвенными дефектами тканей. В основе патологического процесса лежит превалирование альтерационных процессов в слизистой оболочке десен. Ведущую роль в списке причин, приведших к болезни, играет снижение степени устойчивости тканей полости рта к фузосперохетарной микрофлоре.\\nМестные причины:\\n\\t•\\tОслабление иммунной системы после длительной болезни или при ведении нездорового образа жизни.\\n\\t•\\tЧистка зубов с применением неправильной техники\\n\\t•\\tПлохая гигиена зубов и ротовой полости\\n\\t•\\tПрисутствие заболеваний полости рта в запущенной стадии\\n\\t•\\tВозникновение язвенного гингивита как осложнение при прорезывании восьмых коренных зубов.\\nОбщие причины:\\n\\t•\\tЧастые стрессовые ситуации\\n\\t•\\tХроническая нехватка сна\\n\\t•\\tХроническая усталость\\n\\t•\\tНеполноценное питание, при котором организм не получает необходимые питательные вещества, витамины и минералы\\n\\t•\\tНаличие у больного ВИЧ-инфекции или хронических и запущенных заболеваний ССС (сердечнососудистой системы) и эндокринной системы\\n\\t•\\tОстрые отравления организма химическими веществами\\n\\t•\\tИнтоксикация тяжелыми видами металлов.\\nНельзя с точностью сказать, что именно один фактор служит первопричиной развития язвенно-некротического гингивита Венсана. В основе патологии лежит совокупность способствующих факторов или же она сопровождает более серьезное хроническое заболевание организма. [Барер Г.М 2008; Козлов В.А 2003]\\nСимптомы язвенно-некротического гингивита\\nСимптоматика выражена ярко и начинается она с острого периода язвенного гингивита. Такое состояние характеризуется следующими признаками:\\n\\t•\\tСильные боли в пораженной области, кровоточивость десен и гнилостный запах из полости рта.\\n\\t•\\tПовышение температуры, головные боли, наблюдается слабость мышц, пациент становится неработоспособным в период болезни.\\n\\t•\\tВ полости рта обнаруживается зловонный налет, покрывающий десневой край. При удалении налета обнажается болезненная и кровоточащая поверхность слизистой оболочки.\\n\\t•\\tКонтуры десны нечеткие, десневые сосочки (которые в норме имеют форму острого треугольника) трапециевидной формы, без острой вершины.\\n\\t•\\tНа зубах большое количество мягкого зубного налета и твердых зубных отложений.\\n\\t•\\tПациент отмечает появление бессонницы, расстройство работы пищеварительного тракта и потерю аппетита.\\n\\t•\\tПри пальпации поднижнечелюстной области обнаруживаются увеличенные лимфоузлы.\\n\\t•\\tИсследование крови показывает увеличение СОЭ, лейкоцитоз, и элементы белка в моче (в некоторых случаях).\\nВ некоторых случаях язвенный гингивит возникает как осложнение нелеченного катарального гингивита. По статистике, пик обострения симптомов приходится на зимние месяцы, в период наибольшего ослабления организма после перенесенных простудных заболеваний, вирусных инфекций и гриппа.\\nВозможно, внезапно появившийся гингивит Венсана служит первым признаком заражения СПИДом.\\nЗаболевание чаще поражает лиц в подростковом периоде. Именно в юношеском возрасте гигиене полости рта не уделяется достаточно внимания, а посещения стоматолога становятся редким мероприятием.\\n\\n\\nПрофилактика\\nВести здоровый образ жизни, правильно питаться, соблюдать гигиену зубов и ротовой полости. Когда язвенный гингивит является сопутствующим заболеванием, необходимо ликвидировать источник патологических процессов. Для этого нужно пройти тщательное обследование всего организма у терапевта и получить направление к нужному специалисту.\\nВ любом случае, нельзя оставлять признаки язвенного гингивита без внимания. Желательно как можно быстрее пройти лечение, так как данная патология имеет тенденцию к серьезным осложнениям.\\n\\nГИПЕРТРОФИЧЕСКИЙ ГИНГИВИТ\\nГипертрофический гингивит – воспалительные изменения тканей десны, сопровождающиеся их разрастанием (гипертрофией) с образованием ложных зубодесневых карманов, закрывающих зубную коронку. форма хронического гингивита, протекающая с преобладанием пролиферативных процессов в тканях десны. В стоматологии гипертрофический гингивит диагностируется у 3-5% лиц, страдающих заболеваниями пародонта. Развитию гипертрофического гингивита обычно предшествует длительно текущий катаральный гингивит. Гипертрофический гингивит может являться самостоятельным заболеванием или сопутствовать обострению генерализованного пародонтита. При гипертрофическом гингивите, несмотря на существенное увеличение объема тканей десны, целостность зубоэпителиального прикрепления не нарушена, а патологические изменения в костной ткани альвеолы отсутствуют.\\nПричины гипертрофического гингивита\\nВ развитии гипертрофического гингивита могут принимать участие местные и общие факторы. \\n\\n\\n\\nМестные факторы:\\n\\t•\\tАномалии прикуса (Открытый прикус, глубокий прикус).\\n\\t•\\tЗубные отложения (зубной камень, зубной налет).\\n\\t•\\tНизкое прикрепление уздечки.\\n\\t•\\tМеханическая травма десны из-за нависающих краев пломб.\\n\\t•\\tНерационально подобранные ортопедические конструкции.\\n\\t•\\tНеудовлетворительная гигиена при ношении ортодонтических аппаратов.\\nОбщие факторы:\\n\\t•\\tИзменение гормонального статуса (половое созревание, беременность, климакс)\\n\\t•\\tЭндокринные заболевание (сахарный диабет, гипо/гипертериоз)\\n\\t•\\tПрием антиэпилептических лекарственных средств, блокаторов кальциевых каналов, иммуносупрессивных средств, оральных контрацептивов.\\n\\t•\\tГиповитаминоз.\\n\\t•\\tЛейкоз.\\nКлассификация гипертрофического гингивита\\nПо распространенности патологических изменений различают локализованный (в области 1-5 зубов) и генерализованный гипертрофический гингивит. Иногда локализованные поверхностные формы гипертрофического гингивита выделяются в отдельное заболевание - папиллит.\\nВ зависимости от типа гиперпластических процессов гипертрофический гингивит может протекать в отечной и фиброзной форме.\\n\\n\\n\\n\\nОтечна форма (воспалительная)\\n\\t•\\tОтек соединительнотканных волокон десневых сосочков\\n\\t•\\tРасширение сосудов\\n\\t•\\tЛимфоплазмоцитарная инфильтрация тканей десны\\nФиброзная форма (гранулирующая)\\n\\t•\\tПролиферация соединительнотканных волокон десневых сосочков\\n\\t•\\tУтолщение коллагеновых волокон\\n\\t•\\tПаракератоз при минимальной выраженности отека и воспалительной инфильтрации\\n\\nС учетом разрастания тканей десны выделяют три степени гипертрофического гингивита:\\n\\t•\\tЛегкую – гипертрофия десневых сосочков у основания; разросшийся край десны прикрывает коронку зуба на 1/3; \\n\\t•\\tСреднюю – прогрессирующее увеличение и куполообразное изменение формы десневых сосочков; разросшаяся десна наполовину закрывает зубные коронки; \\n\\t•\\tТяжелую - ярко выраженная гиперплазия десневых сосочков и края десны, которая закрывает собой зубную коронку более чем на 1/2 высоты.  [Данилевский Н.Ф; Борисенко А.В 2000]\\nСимптомы гипертрофического гингивита\\nОтечная форма гипертрофического гингивита характеризуется:\\n\\t•\\tЖжение\\n\\t•\\tБолезненность и кровоточивость при чистке зубов и приеме пищи\\n\\t•\\tГипертрофия межзубных сосочков \\n\\t•\\tЯрко-красный цвет десны\\nПри осмотре обнаруживается:\\n\\t•\\tУвеличение и отечность десневых сосочков\\n\\t•\\tГиперемия десневых сосочков\\n\\t•\\tСинюшный оттенок десневых сосочков\\n\\t•\\tГлянцевый блеск\\n\\t•\\tКровоточивость при зондировании\\n\\t•\\tНаличие зубных отложений\\n\\t•\\tНаличие ложных зубодесневых карманов\\n\\t•\\tЦелостность зубодесневого соединения не нарушена\\nПри фиброзном гипертрофическом гингивите жалобы на:\\n\\t•\\tМассивность десен\\n\\t•\\tДесна плотные на ощупь\\n\\t•\\tНеудовлетворительная эстетика\\n\\t•\\tТрудно пережевывать пищу\\nПри осмотре наблюдается :\\n\\t•\\tДесна бледно-розового цвета\\n\\t•\\tБезболезненна\\n\\t•\\tДесна имеет неровную, бугристую поверхность\\n\\t•\\tПри зондировании не кровоточит\\n\\t•\\tНаличие зубных отложений\\n\\n1.3.2 Пародонтит\\nПародонтит - воспалительное заболевание тканей пародонта, характеризующееся прогрессирующим разрушением нормальной структуры альвеолярного отростка челюсти. Пародонтит распространён достаточно широко.\\nКлассификация пародонтита по МКБ-10 (1997 г.):\\nОстрый пародонтит (К05.2):\\nК05.20 – периодонтальный (пародонтальный) абсцесс десневого происхождения без свища;\\nК05.21 – периодонтальный (пародонтальный) абсцесс десневого происхождения со свищом.\\nХронический пародонтит (КО5.3):\\nК05.30 – локализованный;\\nК05.31 – генерализованный;\\nК05.32 – хронический перикоронит;\\nК05.33 – утолщенный фолликул (гипертрофия сосочка).\\nКлассификация по степени тяжести:\\nлегкая – пародонтальные карманы не более 4 мм, резорбция костной ткани межкорневой перегородки до 1/3 длины корней, патологической подвижности нет;\\nсредняя – карманы от 4 до 6 мм, резорбция костной ткани перегородок на 1/3-1/2 длины корней, патологическая подвижность І-ІІ ст.\\nтяжелая – глубина карманов более 6 мм, резорбция костной ткани перегородок более ½ длины корней, патологическая подвижность ІІ-ІІІ ст.\\nПо распространенности:\\nлокализованный (очаговый), генерализованный.\\nСреди причин развития пародонтита выделяют общие (системные) и местные.\\nМестные причины:\\n\\t•\\tМикроорганизмы — наиболее актуальная местная причина пародонтита. Первичным источником их служит зубная бляшка. При тяжелом течении пародонтита чаще всего обнаруживаются Porphyromonas gingivalis, Actinobacillus actinomycetemcomitans, Prevotella intermedia, Treponema denticola.\\n\\t•\\tДругая доказанная группа причин\\xa0— травматическая. К ним относятся: травмирующие аномалии прикуса, высокое прикрепление тяжей и уздечек слизистой полости рта, скученность и аномалии положения зубов, гипертонус жевательной мускулатуры.\\nОбщие причины:\\n\\t•\\tСоматическая патология (сахарный диабет, иммунодефициты, заболевания кровеносной системы и\\xa0пр.), сопровождающаяся так называемым «пародонтальным синдромом».\\n\\t•\\tКроме того, многие хронические заболевания, не обладая специфическим влиянием на пародонт, предрасполагают к возникновению или отягощают течение хронического пародонтита.\\nФакторы риска\\n\\t•\\tНеудовлетворительная гигиена полости рта (наличие зубного камня)\\n\\t•\\tВредные привычки (курение)\\nСимптомы пародонтита\\nПри пародонтите болевой синдром наблюдается редко. Воспаление со стороны десен проявляется отечностью, покраснением, местным повышением температуры и кровоточивостью десен. То есть гингивит является первой стадией. При отсутствии лечения процесс прогрессирует и пародонтитом поражаются мягкие и костные ткани, что может закончиться потерей зуба. \\nДиагностировать пародонтит на ранних сроках можно только во время осмотра стоматологом, так как клинические проявления практически отсутствуют. И основным симптомом, после которого пациенты обращаются за медицинской помощью, является кровоточивость десен во время чистки зубов или во время еды. \\nВ дальнейшем присоединяется отечность десен и их повышенная чувствительность в ответ на раздражение. Если на этом этапе пародонтит не лечить, то десны начинают отделяться от зубов, вследствие чего зубы выглядят длиннее, между зубами появляются промежутки. В последующем при пародонтите появляется гнойное отделяемое и неприятный запах изо рта. Неприятный привкус во рту и выпадение зубов характерны для поздних стадий пародонтита. \\nЛюбой воспалительный процесс в области десен, в том числе и пародонтит, протекает безболезненно вне зависимости от глубины поражения и стадии разрушения пародонтальных тканей. Поэтому даже безболезненная кровоточивость десен является первым клиническим проявлением развивающегося пародонтита. На этом этапе процесс еще обратим, так как пародонтальная связка не вовлечена в воспалительный процесс и зуб связан с соседними зубами, что обеспечивает равномерную нагрузку по всему зубному ряду, в результате перегрузки в тканях пародонта еще не возникает. \\nПри отсутствии лечения пародонтита воспалительный процесс проникает глубже, начинается разрушение пародонтальной связки, возникает пародонтальный карман. Именно в этот карман при пародонтите откладывается зубной налет и зубной камень, что способствует прогрессированию процесса. Далее пародонтальные ткани (десна и костная ткань) разрушаются, зуб начинает расшатываться, теряется костная опора в челюсти. На этом этапе пародонтита положение зубов в зубном ряду меняется, между ними появляются щели. \\nВ зависимости от особенностей пациента, пародонтит протекает по-разному. Так, агрессивное течение пародонтита характеризуется быстрым, почти стремительным разрушением зубов и десен. У другой части пациентов пародонтит протекает эпизодически, с длительными ремиссиями и периодами обострения процесса. \\nХроническое течение пародонтита характеризуется медленным, но прогрессирующим разрушением костной и мышечной тканей, окружающих и поддерживающих зуб. Хронический пародонтит имеет более медленное течение, чем агрессивный. Если пародонтит является одним из проявлений системных заболеваний, например сахарного диабета, то обычно его симптомы проявляются в раннем возрасте и стихают во время коррекции основного заболевания. \\nНекротизирующий пародонтит является наиболее тяжелой формой течения заболевания. Десневые ткани некротизируются, при отсутствии лечения наблюдается некроз периодонтальных связок и костной ткани. Некротизирующий пародонтит встречается в основном у пациентов с тяжелыми формами иммунодефицитов, например, больные СПИДом. \\n\\nПрофилактика\\nСанитарное просвещение населения, обучение правильной чистке зубов еще в детском возрасте и полноценное питание. Для своевременного выявления пародонтита и других заболеваний необходимо не реже, чем раз в полгода проходить плановый осмотр стоматолога и процедуры по удалению зубного камня, который провоцирует развитие пародонтита. \\nЕсли имеется гингивит, то его необходимо лечить, так как именно гингивит является первым проявление пародонтита. На этом этапе предотвратить пародонтит можно используя антибактериальные зубные пасты и противовоспалительные ополаскиватели для рта. Дефекты зубных рядов необходимо своевременно лечить, так как это формирует правильную нагрузку на зубы и способствует профилактике. [Вишняк Г.Н 1999; Борисенко А.В., Данилевский Н.Ф 2000; Максимовская Л.Н 2001; Мюллер Х.П 2004]\\n\\n1.3.3 Пародонтоз\\nПародонтоз\\xa0— атрофические процессы в пародонте, приводящие к нарушению единства связочного аппарата зуба с костной тканью (альвеолярными отростками челюсти). Характеризуется прогрессирующим течением, проявляется ощущением дискомфорта в деснах, подвижности зубов, неприятного запаха и вкуса во рту. В дальнейшем шейки зубов обнажаются, происходит образование клиновидных дефектов на эмали. Встречается относительно редко, не чаще чем у 1—8\\xa0% пациентов.\\nПатогенез\\nЗаболевание проявляется прогрессирующей атрофией зубных ячеек альвеолярных отростков. Рентгенологическое исследование позволяет выявить склеротические изменения костной ткани (уменьшение костномозговых пространств, мелкоячеистый рисунок кости). Атрофические процессы в этой ткани приводят к равномерному уменьшению высоты межзубных перегородок при сохраняющихся кортикальных пластинках. При рентгенологическом исследовании определяется убыль костной ткани межзубных перегородок, очаги остеопороза, рисунок кости мелкоячеистый, склеротированный.\\nЭтиология\\nПричины пародонтоза точно не установлены, считается, что важную роль играет наследственная предрасположенность. Часто возникает при системных заболеваниях, сахарном диабете и др. нарушениях деятельности желёз внутренней секреции, при хронических заболеваниях внутренних органов (атеросклероз, гипертония, вегетососудистая дистония), а также поражениях костей (остеопении).\\nПричины развития пародонтоза\\nОсновной причиной пародонтоза являются патогенные микроорганизмы, которые находятся в зубном налете. В результате их жизнедеятельности ткань десны становится рыхлой, разрушается зубодесневое соединение и зубной налет проникает глубже. После затвердевания зубной налет повреждает десну и зубную эмаль. Пародонтоз чаще встречается у людей с патологиями сердечно-сосудистой системы атеросклеротического характера, с патологиями пищеварительного тракта и эндокринными заболеваниями.\\nНарушения обмена веществ, особенно вследствие гиповитаминоза становятся причиной пародонтоза в старшем возрасте, когда естественное ослабление тканей пародонта более выражено. В патогенезе пародонтоза кроме микробного фактора и дистрофических изменений лежат также аномалии развития зубочелюстной системы. Так, при патологиях прикуса, аномалиях положения зубов пародонтоз диагностируется в несколько раз чаще. \\u2028Симптомы пародонтоза\\nПародонтоз является длительно текущим заболеванием десен, которое со временем приводит к появлению неприятного запаха изо рта и к выпадению зубов. Пародонтоз и другие заболевания десен являются основной причиной потери зубов в зрелом возрасте. При пародонтозе образуется глубокий десневой карман, в котором скапливаются остатки пищи, микроорганизмы и зубной налет. Следовательно, именно такие воспаленные десневые карманы и являются источником зловонного запаха изо рта и неприятного привкуса во время еды. \\nПовышенная кровоточивость десен при пародонтозе во время чистки зубов или принятия пищи объясняется разрыхленностью десен. Кроме того повышается чувствительность десен ко всем типам раздражителей, они становятся болезненными и легко воспаляются. Когда зубодесневое соединение полностью разрушается и десневой карман становится глубоким и окончательно сформированным, наступает этап формирования микробной бляшки на корне зуба. Это заключительная стадия пародонтоза, на которой рассасывается корень зуба и челюстная кость, развивается грануляционная ткань, что приводит к потере зубов. \\nПатогенные микроорганизмы провоцируют развитие пародонтоза, но не являются его основной причиной, так как в части случаев патологический процесс при пародонтозе носит неинфекционный характер. После формирования десневого кармана шейки зубов обнажаются. Этот этап пародонтоза длительный, и, несмотря на почти полное обнажение шеек зубов, зубы длительное время хорошо сохраняют фиксацию. \\nПо мере прогрессирования присоединяется повышенная чувствительность шеечной части зуба и ощущение зуда в деснах, иногда при пародонтозе может быть ярко выраженное воспаление десны. \\nВ зависимости от того, насколько явно выражены симптомы, различают несколько степеней тяжести пародонтоза.\\n\\t•\\tПри легкой форме пародонтоза внутридесневая часть зуба обнажена примерно на треть\\n\\t•\\tПри средней и тяжелой форме обнажение зуба происходит наполовину и более. \\n[Мюллер Х.П. 2004; Орехова Л.Ю. 2004, Иванов В.С. 1998; Цепов Л.М. 2008]\\nЕсли патологический процесс принял необратимый характер, то есть появились признаки атрофии десен и подвижности зуба, то возможна потеря зубов. Часто пародонтоз сочетается с некариозными заболеваниями зубов – эрозией эмали, стертостью зуба, клиновидным дефектом.  [Иванов В.С. 1998; Цепов Л.М 2008]\\n\\nГлава2. МАТЕРИАЛЫ И МЕТОДЫ ИССЛЕДОВАНИЯ.\\n\\n2.1 Характеристика обследованных пациентов.\\nОбъектом исследования стали пациенты с пародонтологическими заболеваниями, которые проходили лечение в «СПБ ГБУЗ стоматологическая городская поликлиника № 33» пародонтологическом отделение. Исследование проводилось на базе «СПБ ГБУЗ стоматологическая городская поликлиника № 33».\\nВ исследовательской работе принимали участие 63 пациента, из них 29 мужчин и 34 женщины. Всем пациентам было проведено обследование, предусматривающее оценку стоматологического статуса, с занесением полученных данных в карту обследования стоматологического пациента Было сформировано 4 группы наблюдения в соответствии с возрастом.  К первой группе относятся пациенты до 25 лет. Во второй группе пациенты  25-44 лет. К третьей группе относятся пациенты в возрасте  44-60 лет.  И в четвертой группе пациенты 60-75 лет.\\nГруппировка и шифровка материалов\\n№\\nВозрастные группы\\nЧисло пациентов\\n1\\nДо 25 лет\\n11\\n2\\nОт 25-44 лет\\n26\\n3\\nОт 44-60 лет\\n19\\n4\\nОт 60-75 лет\\n7\\n2.2 Оценка стоматологического статуса пациентов\\nМетодика клинического обследование пациентов заключалось в осмотре внешнего вида лица, зубных рядов,  пародонта, слизистой оболочки. \\n\\xa0\\xa0I. Основные методы исследования:\\n\\t•\\tСбор анамнеза жизни и заболевания\\n\\t•\\tЧто беспокоит пациента, на что жалуется\\n\\t•\\tУстановить сущность основной жалобы (боль, дискомфорт, неприятные ощущения; эстетическая проблема; нарушение функций; неприятный запах из полости рта)\\n\\t•\\tОпределить когда пациент впервые отметил появление беспокоящей его проблемы, постоянно или периодически возникают неприятные ощущения, есть ли облегчающие факторы, с момента возникновения заболевания наблюдается улучшение/ухудшение или сохраняется в таком же состоянии.\\n\\t•\\tЕсли основная жалоба боль, то следуют выяснить локализацию, характер, иррадиацию.\\n\\t•\\tКлинический осмотре полости рта\\n\\t•\\tОсмотр красной каймы губ и углов рта.\\n\\t•\\tОсмотр тканей преддверия полости рта (оцениваем состояние зубных рядов и пародонта, слизистую оболочку собственно полости рта; глубину преддверия полости рта, цвет слизистой оболочки, выраженность уздечек и уровень их прикрепления на альвеолярном отростке.)\\n\\t•\\tОсмотр десен (оцениваем цвет, консистенцию, контур и расположение десневого края, размеры, кровоточивость, болезненность) \\n\\t•\\tОсмотр зубных рядов (оцениваем взаимоотношение зубов, наличие зубных отложений, степень стираемости, наличие кариозных полостей и дефектов некариозного происхождения, качество пломб, качество ортопедических конструкций)\\n\\t•\\tОсмотр слизистой оболочки полости рта и языка\\n\\t•\\tОпределение наличия зубных отложение с помощью индекса Федорова-Володкиной\\nГигиенический индекс по методу Ю.А. Федорова и В.В. Володкиной\\nДля определения индекса окрашивают вестибулярные поверхности фронтальных зубов нижней челюсти 3.1, 3.2, 3.3, 4.1, 4.2, 4.3 раствором Шиллера-Писарева \\nПроизводится количественная и качественная оценка\\nКоличественная оценка:\\n1 балл: отсутствие окрашивания\\n2 балла: окрашивание поверхности коронки зуба на ¼\\n3 балла: окрашивание поверхности коронки зуба на ½\\n4 балла: окрашивание поверхности коронки зуба на 2/3\\n5 баллов: окрашивание всей поверхности коронки зуба\\nДля вычисления индекса делят сумму значений индекса у всех окрашенных зубов на количество обследованных зубов. В норме индекс не более 1\\nКачественная оценка:\\n\\t•\\t1.1-1.5 балла – хороший ИГ\\n\\t•\\t1.6-2 балла – удовлетворительный ИГ\\n\\t•\\t2.1-2.5 балла – неудовлетворительный ИГ\\n\\t•\\t2.6-3.4 балла – плохой ИГ\\n\\t•\\t3.5-5 баллов – очень плохой ИГ\\n\\t•\\tПародонтальный индекс CPITN для оценки распространенности и интенсивности заболеваний пародонта\\nЭто комплексный пародонтальный индекс используют, для того чтобы оценить состояние тканей пародонта взрослого населения, для  последующего планирования разработки программ профилактики и методов лечения. При оценке состояния тканей применяют специальный пародонтальный зонд, который имеет на конце шарик диаметром 0.5мм и черную полоску на расстоянии 3.5мм от кончика зонда. У пациентов исследуют периодонт в области данных зубов 1.7/1.6, 1.1, 2.6/2.7, 3.7/3.6, 3.1, 4.6/4.7. Для критерия оценки используют следующие коды:\\n\\t•\\t0 – здоровые десны;\\n\\t•\\t1 – возникновение кровоточивости десен при их зондировании;\\n\\t•\\t2 – при зондировании определяются поддесневые зубные отложения, черная полоска зонда не погружается в десневой карман;\\n\\t•\\t3 – наличие карман от 4 до 5мм, черная полоска зонда частично погружается в зубо-десневой карман; \\n\\t•\\t4 – наличие кармана > 6мм, черная полоска зонда полностью погружена в десневой карман.\\n\\t•\\tПапиллярно-маргинально-альвеолярный индекс РМА, для оценки степени тяжести гингивита\\nПапиллярно-маргинально-альвеолярный индекс (PMA).\\nКоды:\\n\\t•\\t0 — нет признаков воспаления десны;\\n\\t•\\t1 — воспаление десневого сосочка (Р);\\n\\t•\\t2 — воспаление маргинальной десны (М);\\n\\t•\\t3 — воспаление альвеолярной десны (А). \\nИндекс РМА рассчитывают по формуле:   \\xa0\\xa0\\xa0\\nРМА\\xa0= (Сумма баллов/ 3*количество зубов)* 100%\\nКритерии индекса РМА:\\n\\t•\\tдо 30%— легкая степень тяжести гингивита;\\n\\t•\\t31—60 % — средняя степень тяжести;\\n\\t•\\t> 61% — тяжелая степень.\\n\\t•\\tХарактер экссудата из пародонтального кармана\\n\\t•\\tОценку рецессии десны по шкале Miller (1985):\\n1 класс. Рецессия в пределах свободной десны. Утрата десны и/или кости в межзубных промежутках отсутствует\\n2 класс. Рецессия в пределах прикрепленной десны. Утрата десны и/или кости в межзубных промежутках не отсутствует\\n3 класс. К рецессии 2-го класса добавляется поражение аппроксимальной поверхности\\n4класс. Наблюдается циркулярная потеря десны и кости в межзубных промежутках\\n\\t•\\tОценка подвижности зубов по степени их смещения по шкале Miller в модификации Fleszar (1980)\\n0 – зуб устойчив, подвижность находится в пределах физиологической\\n1 степень – зуб смещается относительно оси, но смещение не превышает 1 мм.\\n2 степень – зуб смещается на 1-2 мм в вестибуло-оральном направлении, при этом функция зуба не нарушается.\\n3 степень – подвижность резко выражена, зуб подвижен не только в вестибуло-оральном направлении, но и вертикально, нарушается функция зуба\\n9. Исследование клинических карманов.\\nДля исследования клинических карманов используются пародонтальные зонды. Рабочая часть пародонтального зонда заканчивается тупо или имеет маленький шарик на конце диаметром 0,5 мм. Пародонтальные зонды маркированы по миллиметровой шкале.\\nМетодика измерения глубины пародонтального кармана:\\nПародонтальный зонд осторожно вводится в карман или бороздку исследуемого зуба. Рабочая часть зонда ориентируется вдоль длинной оси зуба перпендикулярно десневому краю при постоянном контакте с корнем. Следуя по анатомической поверхности корня зуба, конец зонда мягко продвигается между зубом и десной до тех пор, пока не станет ощутимым сопротивление надальвеолярных волокон. Глубина клинических карманов измеряется с четырех сторон зуба (дистальной, медиальной, вестибулярной, язычной или небной).\\nII. Дополнительные методы исследования \\nДля выявления и оценки патологических изменений в костной ткани в настоящее время широко применяются рентгенологические методы исследования. Определенное диагностическое значение имеет эхоостеометрия. Изменения гомеостаза костной ткани, нарушение обмена кальция, фосфора, повышенный̆ лизис коллагена и т.п. выявляются при биохимических исследованиях жидких сред организма. \\nРентгенологический метод занимает особое место в диагностике заболеваний пародонта не только вследствие доступности, но и потому, что дает возможность судить как о степени поражения костной ткани, так и о характере, стадии и тяжести патологического процесса. \\nПри рентгенологическом исследовании пародонта наиболее часто используют внутриротовую контактную рентгенографию альвеолярных отростков, панорамную рентгеографию и ортопантомографию челюстей. \\nВнутриротовой контактный метод позволяет получить четкое изображение структуры костной ткани на ограниченном участке альвеолярного отростка в области 3-5 зубов. \\nОртопантомография (панорамная томография) позволяет получить изображение обеих челюстей на одной пленке. Исследование проводится на ортопантомографе. \\n2.3 Оценка рентгеновских снимков\\nПри анализе рентгенограмм мы обращали внимание на форму, высоту, состояние вершин межальвеолярных перегородок, степень минерализации губчатого вещества, со\\xad стояние кортикального слоя. \\nПри рентгенологическом исследовании здоровой кости альвеолярного отростка кортикальный слой альвеолярного края и лунок проявляется непрерывной белой полоской, отчетливо выраженной на вершинах межзубных перегородок и не всегда четкой в области зубов из-за накладывающихся теней. Губчатая ткань кости представляется наподобие сетки переплетенных светлых полосок (костные балки) и различной величины темных пространств. \\nОчаги патологически измененной костной ткани оценивали по следующим показателям: количество очагов, их локализация в кости, форма, размеры, контуры, интенсивность тени, состояние костной ткани в самом очаге и вокруг него. \\nПри заболеваниях пародонта наиболее часто выявляются следующие патологические изменения в костной ткани. \\nОстеопороз — дистрофический процесс в костной ткани, рентгенологически проявляющийся ее повышенной прозрачностью, с уменьшением количества костной ткани на единицу площади без изменения размеров кости. \\nДеструкция — разрушение кости и замещение ее патологической тканью (грануляциями, гноем, опухолью); на рентгенограмме очаг деструкции представлен в виде участка просветления с нечеткими, неровными контурами. \\nАтрофия — уменьшение объема всей кости или ее части вместе с убылью костной ткани. \\nОстеосклероз — процесс противоположный остеопорозу, увеличение количества костной ткани на единицу площади без изменения размеров кости, рентгенологически проявляющийся снижением прозрачности костной ткани. \\n2.4. Параклинические методы.\\nДля обработки полученных данных в ходе исследования, мы использовали математический метод.\\nПолученные нами результаты обрабатывались с помощью статистической программы. Статистическая обработка включала вычисление параметров средних величин и их отклонений, достоверности отличий с использованием критерия Стьюдента (достоверность различий р<0,05) в программе Microsoft Exel. Для визуализации результатов исследования были построены диаграммы.\\n\\nГЛАВА 3. РЕЗУЛЬТАТЫ ИССЛЕДОВАНИЯ\\n\\n3.1 Результаты клинического исследования\\nДля решения поставленных задач было проведено обследования 63 пациентов в возрасте от 25 до 75 лет\\nВ ходе клинического исследования было выяснено, что все обследуемые пациенты предъявляют жалобы на кровоточивость при чистке зубов, кровоточивость при приеме пищи, неприятный запах из полости рта, зуд и жжение в деснах, отек и воспаление десен, попадание пищи между зубов.\\nАнализ жалоб по группам представлен в таблице 3.1.1\\n\\nТаблица 3.1.1 Жалобы пациентов разных возрастных групп\\nГруппа\\nЖалоба \\n1 группа\\n2 группа\\n3 группа\\n4 группа\\n\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nКровоточивость при чистке зубов\\n11\\n100\\n26\\n100\\n19\\n100\\n7\\n100\\nКровоточивость при приеме пищи\\n6\\n54\\n13\\n50\\n3\\n16\\n1\\n14\\nНеприятный запах из полости рта\\n8\\n73\\n18\\n69\\n19\\n100\\n7\\n100\\nЗуд и жжение в деснах\\n3\\n27\\n15\\n58\\n8\\n42\\n7\\n100\\nОтек и воспаление десен\\n11\\n100\\n26\\n100\\n19\\n100\\n4\\n57\\nПопадание пищи между зубами\\n6\\n54\\n13\\n50\\n3\\n16\\n1\\n14\\nУхудшение общего состояния организма\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\nР<0,05\\n\\nРис. 3.1.1 Жалобы пациентов в разных возрастных группах\\nПри сборе анамнеза были установлены соматические заболевания: гипертоническая болезнь, гипотония, бронхиальная астма, заболевание щитовидной железы, эпилепсия, сахарный диабет, заболевание почек, заболевание ЖКТ, лекарственная непереносимость. \\n\\nСоматические заболевания у пациентов представлены в таблице 3.1.2\\nТаблица 3.1.2\\nЗаболевания\\nЧисло пациентов\\n%\\nГипертоническая болезнь\\n21\\n33\\nГипотония\\n5\\n8\\nБронхиальная астма\\n12\\n19\\nЗаболевание щитовидной железы\\n8\\n13\\nСахарный диабет\\n9\\n14\\nЗаболевание почек\\n13\\n21\\nЗаболевание ЖКТ\\n17\\n27\\nЛекарственная непереносимость\\n18\\n28\\n\\n\\n\\n\\nАнализ соматических заболеваний у пациентов представлен в таблице 3.1.3\\nТаблица 3.1.3\\n    Группы\\n\\nЗаболевания \\n1 группа\\n2 группа\\n3 группа\\n4 группа\\n\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nГипертоническая болезнь\\n0\\n\\n3\\n11\\n11\\n58\\n7\\n100\\nГипотония\\n4\\n36\\n0\\n\\n1\\n5\\n0\\n\\nБронхиальная астма\\n2\\n18\\n7\\n27\\n0\\n\\n3\\n42\\nЗаболевание щитовидной железы\\n2\\n18\\n1\\n4\\n0\\n\\n5\\n71\\nСахарный диабет\\n0\\n\\n4\\n15\\n4\\n21\\n1\\n14\\nЗаболевание почек\\n3\\n27\\n3\\n11\\n7\\n37\\n0\\n\\nЗаболевание ЖКТ\\n4\\n36\\n6\\n23\\n2\\n10\\n5\\n71\\nЛекарственная непереносимость\\n10\\n90\\n2\\n8\\n3\\n16\\n3\\n42\\n\\n\\n\\n\\n\\n\\n\\nP<0,05\\n\\nПри оценке полученных данных по гигиеническим навыками пациентов (таблица 3.1.4), было выяснено, что большинство пациентов (95%) чистит зубы 1 раз в день, 5% чистит зубы 2 раза в день утром и вечером.\\n\\nТаблица 3.1.4\\nНавыки индивидуальной гигиены полости рта обследованных пациентов\\n\\nРегулярность чистки зубов\\nЧисло пациентов\\n%\\n1 раз в день\\n60\\n95\\n2 раза в день утром и вечером после еды\\n3\\n5\\n\\nПри осмотре полости рта было проведено определение зубных отложений с помощью индекса Федорова-Володкиной (Таблица 3.1.5)\\nТаблица 3.1.5\\n    Группы\\n\\nИндекс гигиены \\n1 группа\\n2 группа\\n3 группа\\n4 группа\\n\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nХороший\\n7\\n63\\n1\\n4\\n-\\n-\\n-\\n-\\nУдовлетворительный\\n3\\n27\\n3\\n11\\n12\\n63\\n-\\n-\\nНеудовлетворительный\\n-\\n-\\n10\\n38\\n7\\n37\\n5\\n71\\nПлохой\\n-\\n-\\n12\\n46\\n-\\n-\\n2\\n18\\nОчень плохой\\n1\\n9\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n\\n\\n\\n\\n\\n\\nP<0,05\\n\\n\\n\\nДля оценки состояния тканей пародонта использовался пародонтальный индекс CPITN (таблица 3.1.6)\\nТаблица 3.1.6\\n    Группы\\n\\nКритерий \\n1 группа\\n2 группа\\n3 группа\\n4 группа\\n\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЗдоровые десны\\n1\\n9\\n-\\n-\\n-\\n-\\n-\\n-\\nКровоточивость десен при зондировании\\n10\\n90\\n26\\n100\\n19\\n100\\n7\\n100\\nПри зондировании поддесневые зубные отложения\\n11\\n100\\n26\\n100\\n19\\n100\\n3\\n43\\nПародонтальный карман от 4 до 5 мм\\n-\\n-\\n13\\n50\\n6\\n31\\n-\\n-\\nПародонтальный карман более 6 мм\\n-\\n-\\n7\\n27\\n-\\n-\\n7\\n100\\nР<0,05\\n\\n\\n\\n\\nДля оценки степени тяжести гингивита был использован папиллярно-маргинально-альвеолярный индекс РМА (Таблица 3.1.7)\\nТаблица 3.1.7\\n    Группы\\n\\nКритерий  \\n1 группа\\n2 группа\\n3 группа\\n4 группа\\n\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЛегкая степень тяжести гингивита\\n11\\n100\\n1\\n4\\n12\\n63\\n-\\n-\\nСредняя степень тяжести\\n-\\n-\\n13\\n50\\n7\\n37\\n5\\n71\\nТяжелая степень\\n-\\n-\\n12\\n46\\n-\\n-\\n2\\n29\\nP<0,05\\n\\nТакже была проведена оценка рецессии десны по шкале Miller (таблица 3.1.8)\\nТаблица 3.1.8\\n    Группы\\n\\nКласс  \\n1 группа\\n2 группа\\n3 группа\\n4 группа\\n\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\n1 класс\\n8\\n72\\n4\\n16\\n-\\n-\\n-\\n-\\n2 класс\\n3\\n28\\n12\\n46\\n10\\n53\\n-\\n-\\n3 класс\\n-\\n-\\n10\\n38\\n7\\n37\\n-\\n-\\n4 класс\\n-\\n-\\n-\\n-\\n2\\n10\\n7\\n100\\n P<0,05\\n\\nБыла проведена оценка подвижности зубов по степени их смещения (таблица 3.1.9)\\nТаблица 3.1.9\\n    Группы\\n\\nСтепень\\n1 группа\\n2 группа\\n3 группа\\n4 группа\\n\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\n0\\n11\\n100\\n4\\n15\\n-\\n-\\n-\\n-\\n1\\n-\\n-\\n10\\n38\\n3\\n16\\n-\\n-\\n2\\n-\\n-\\n12\\n46\\n12\\n63\\n3\\n43\\n3\\n-\\n-\\n-\\n-\\n4\\n21\\n4\\n57\\nP<0,05\\n\\nСтруктура заболеваний пародонта у обследованных пациентов (Таблица 3.1.10)\\nТаблица 3.1.10\\n    Группы\\n\\nСтепень выраженности заболеваний тканей пародонта\\n1 группа\\n2 группа\\n3 группа\\n4 группа\\n\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nОтсутствует\\n4\\n36\\n-\\n-\\n-\\n-\\n-\\n-\\nЛегкая \\n5\\n45\\n10\\n38\\n3\\n16\\n-\\n-\\nСредняя\\n2\\n19\\n15\\n58\\n10\\n53\\n2\\n28\\nТяжелая\\n-\\n-\\n1\\n4\\n6\\n31\\n5\\n72\\nP<0,05\\n\\n\\n\\n\\n\\n\\n\\n3.2 Результаты рентгенологического исследования\\nПри рентгенологическом исследовании были выявлены пародонтальные карманы, наблюдалась клиническая потеря прикрепления, деструкция костной ткани соответственно степени тяжести ХГП\\nПри исследовании пародонтальных карманов у пациентов разных возрастных групп с помощью пародонтального зонда и оценки анализа рентгеновских снимков было получено следующее: у пациентов первой группы не было обнаружено пародонтальных карманов глубиной более 5 мм, у пациентов второй, третей и четвертой группы наоборот не было обнаружено пародонтальных карманов глубиной до 3 мм.  Результаты приведены в таблице 3.2.1\\nТаблица 3.2.1\\n    Группы\\n\\nГлубина пародонтального кармана\\n1 группа\\n2 группа\\n3 группа\\n4 группа\\n\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nЧисло пациентов\\n%\\nДо 3 мм\\n7\\n63\\n-\\n-\\n-\\n-\\n-\\n-\\n3-5 мм\\n4\\n37\\n23\\n88\\n10\\n53\\n5\\n71\\nБолее 5 мм\\n-\\n-\\n3\\n12\\n9\\n47\\n2\\n29\\nP<0,05\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nЗАКЛЮЧЕНИЕ\\nЦелью нашего исследования являлось проведение аналитического исследования пародонтологических заболеваний у пациентов разных возрастных групп.\\nВ ходе исследования были изучены структура заболеваний пародонта и соматический статус пациентов разных возрастных групп.\\nВ исследовании приняли участие 63 пациента в возрасте до 75 лет. Были собран жалобы пациентов, анамнез. Использованы клинические и рентгенологические исследования.\\nПри анализе клинических данных было выявлено, что все обследованные пациенты разных возрастных групп предъявили жалобы на кровоточивость при чистке зубов (100%),  на отек и воспаление десен (100%), процент предъявляемых жалоб увеличивается с возрастом пациента. \\nПри анализе индексов гигиены и состояния тканей пародонта было выявлено, что гигиена полости рта и состояние тканей пародонта у обследованных пациентов значительно ухудшается с возрастом.\\nПри оценке данных рентгенологического исследования было выявлено: деструкция костной ткани, компактной пластинки альвеолярного гребня, количество костных карманов и периапикальных изменений. \\nВ ходе исследования были изучены методы профилактики пародонтологическх заболеваний у пациентов разных возрастных групп с учетом соматического статуса\\n\\n\\n\\n\\n\\nВыводы:\\n\\n\\t•\\tУ пациентов старших возрастных групп наблюдается более тяжелое поражение тканей пародонта по сравнению с пациентами молодого возраста.\\n\\t•\\tУ пациентов разных возрастных групп наблюдается существенные различия в соматическом статусе. В старших возрастных группах отмечено наличие сочетанных патологий, а также возрастание степени их тяжести.\\n\\t•\\tБала выявлена тесная взаимосвязь наличия  степени тяжести соматических патологий и степени выраженности поражения тканей пародонта.\\n\\t•\\tПациентам с заболеваниями пародонта необходимо проводить меры профилактики заболевания с учетом их соматического статуса и возрастных групп.\\n\\n\\n\\n\\n\\n\\n\\n\\nПрактические рекомендации:\\n\\nПри проведении мер профилактики заболеваний пародонта необходимо учитывать соматический статус и возрастную группу пациентов. \\nПри планировании и осуществлении комплексного лечения тканей пародонта первым этапом следует проводить профессиональную гигиену полости рта, которая включает в себя : обучение гигиене полости рта, контролируемую чистку зубов, снятие мягких и твердых зубных отложений, индивидуальный подбор предметов и средств гигиены. \\nВ дальнейшем методы лечения заболевания пародонта должны учитывать соматический статус и возрастную группу пациентов. Пациенты старших возрастных групп с соматическими заболеваниями нуждаются в диспансерном наблюдении (срок устанавливается в зависимости от тяжести заболевания)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nСписок использованной литературы\\n\\t•\\tБоровский Е.В., B.C. Иванов, Ю.М. Максимовский, Л.Н. Максимовская Терапевтическая стоматология  2001334 стр\\n\\t•\\tБезрукова И. В., А.И. Грудянов. Состояние местных и общих защитных факторов при заболеваниях пародонта 1987. 4 стр\\n\\t•\\tБыков А.С., Е.П. A.M. Рыбакова Микробиология.2003. – 335-338 с.\\n\\t•\\tВаршавский А. И.  Состояние микроциркуляторного русла пародонта при пародонтозе    стоматология. 1977 ;72-74 с.\\n\\t•\\tГаврилов Е.И.   Язвенно-некротические стоматиты у больных с нарушениями\\xa0кровообращения 635 стр\\n\\t•\\tГригорьян А.С.,  О.А.   Фролова.Диагностика в пародонтологии  – Москва: МИА, 2004 107 стр\\n\\t•\\tДанилевский Н.Ф. Заболевания пародонта 2000г 187 стр\\n\\t•\\tКруглова Н.В «Оценка эффективности комплексного лечения воспалительных заболеваний пародонта»: диссертация 15 стр\\n\\t•\\t15) Лукиных Л.М. Заболевания слизистой оболочки полости рта –2000г. 148 стр\\n\\t•\\tМюллер П.  Пародонтология, 2004. 257 стр\\n\\t•\\tФедоров Ю.А., В.В.  Оценка очищающего действия зубных гигиенических средств и качества ухода за полостью рта 125 стр\\n\\t•\\tЦимбалистов А.В.Комплексное лечение генерализованного пародонтита 2008 г. 38 стр\\n\\t•\\tЦепов. Л.М.  Практическая терапевтическая стоматология: учебн. пособ. для студентов медицинских вузов 2006 272 стр\\n\\t•\\tЦепов Л. М., Николаев А. И., Михеева Е. А. Диагностика, лечение и профилактика заболеваний пародонта, 3-е изд. -  Москва,  2008 549 стр\\n\\t•\\tЧурилов Л.П., А.И. Каспина. Механизмы развития стоматологических заболеваний: учеб. Пособие 2006 437 стр\\n\\t•\\tУлитовский С.Б. Гигиенический уход при воспаленном пародонте. Москва, 2008 28 стр\\n\\t•\\tОрехова Л. Ю. Заболевания пародонта. –  Москва, 2004 432 стр\\n\\t•\\tМаксимовский Ю. М., Дмитриева Л. А. Терапевтическая стоматология. Национальное руководство. – Москва, 2009. 912 стр\\n\\t•\\tИванов В. С. Заболевания пародонта, 3-е изд. – Москва, 1998. 296 стр\\n\\t•\\tДмитриева Л. А. Пародонтит. – Москва, 2007 504 стр\\n\\t•\\tГерберт Ф. Вольф, Эдит М. Ратейцхак, Клаус Ратейцхак. Пародонтология. По ред. проф. Г.М. Барера. – Казань,2007. 548 стр\\n\\t•\\tБарер Г.М. Терапевтическая стоматология, часть 2. Заболевания пародонта. -   Москва,  2013 225 стр\\n\\t•\\tГрудянов А.И., Овчинникова В.В. Профилактика воспалительных заболеваний пародонта. – Москва, 2007. 80 стр \\n\\t•\\tГригорьян А. С., Грудянов А. И., Рабухина Н. А. Болезни пародонта: Патогенез, диагностика, лечение. – Москва, 2004 320 стр\\n\\t•\\tЛукиных Л.М. Болезни пародонта (клиника, диагностика, лечение и профилактика) : руководство / Л.М. Лукиных, Е.Н. Жулев, И.Н. Чупрунова. – Нижний Новгород: НГМА, 2005. 322стр\\n\\t•\\tГерберт Ф. Воьф, Эдит М. Ратейцхак, Клаус Ратейцхак Пародонтология 2008г, 15 стр\\n\\t•\\tБелоклицкая Г.Ф. Клинические формы генерализованного пародонтита и их значение для его дифференциальной терапии 1998 №3 16-20 стр\\n\\t•\\tКурякина Н.В. Заболеване пародонта Медицинсая книга; Н.Новгород издательство НГМА 2000г, 160 стр\\n\\t•\\tМащенко И.С. Заболевание пародонта 2003 г, 271 стр\\n\\t•\\tКильмухаметова Ю. Х., Батиг В. М., Абрамчук И. И. Заболевания пародонта на фоне соматических патологий  57 стр\\n\\t•\\tЗаболевание пародонта под редакцией проф. Л.Ю. Ореховой 2004 142 стр\\n\\t•\\tЧелидзе.А.Н.  Системные\\xa0стоматологические\\xa0заболевания 1984; 127 с.\\n\\t•\\tElders P.J., U. Habets, J.C. Netelen The relation between periodontitis and systemic bone mass in women between 46 and 55 years of age. 2002. P. 382-387.\\n\\t•\\tAlbander J.M., De Nardin E. Serum Ig G level to P. Gingivalis in healthy and early-onset periodontitis individuals. J. Dent. Res. 1999. Vol.78: 250-255.\\n\\t•\\tHaerian A, Akbari S, Ahmadzade M, et al. Comparison of periodontal destruction, between patients with coronary heart disease and healthy subjects, using panoramic radiography. Journal of Periodontology &Implant Dentistry. 2012;\\n\\t•\\tPetersen PE, ogawa H. Strengthening the Prevention of Periodontal Disease: The WHO Approach. Jperiodontal. 2005; 76(12): 2187-93.\\n\\t•\\tVan der Velden U. Purpose and problems of periodontal disease classification. Periodontol 2000. 2005;39:13-21\\n\\t•\\tSocransky S.S.  Microbial etiological agents of destructive periodontal diseases. Periodontol 2000P.67-98\\n\\t•\\tArmitage GC 1999 Development of a classification system for periodontal diseases and conditions. Annals of Periodontology 4:1–6 \\n\\t•\\tGreene JC, Vermillion JR 1964 The simplified oral hygiene index. Journal of the American Dental Association 68:7–13 \\n\\t•\\tSilness J, Loe H 1964 Periodontal disease in pregnancy. II Correlation between oral hygiene and periodontal condition. Acta Odontologica Scandinavica 22:121–135 \\n\\t•\\tLoe H 1967 The Gingival Index, the Plaque Index, and Retention Index system. Journal of Periodontology 38:610–616 \\n\\t•\\tCarranza, F. A. Carranza’s Clinical Periodontology / F. A. Carranza. 11th ed. Saunders Elsevier, 2012. \\n\\t•\\tAesthetic Periodontology / J. L. Denisova [et al.]. Minsk : BSMU, 2015 \\n\\t•\\tSchluger, S. Periodontal diseases : basic phenomena, clinical management and occlusal and restorative interrelationships / S. Schluger, R. Yuodelis, R. C. Page. 2nd ed. Philadelphia : Lea & Febiger, 1990.\\n\\t•\\tFlemmig TF. Periodontitis. Ann Periodontol 1999;4:32-8 \\n\\t•\\tRowland RW. Necrotizing Periodontol 1999;4:65-73 \\n\\t•\\tTonetti MS, Mombelli A. Early-onset periodontitis. Ann Periodontol 1999;4:39-53 \\n\\t•\\tDarby I, Curtis M. Microbiology of periodontal disease in children and young adults. Periodontol 2000, 2001;26:33- 53. \\n\\n\\n\\n\\n### Response:Здоровье полости рта, как считают специалисты, представляет собой одну из главных основ общего здоровья человека. Официальная статистика ВОЗ гласит, что примерно 98% людей во всем мире страдают воспалительными заболеваниями тканей пародонта.   Распространенность заболеваний пародонта в России, в зависимости от возраста, колеблется от 48,2% (в 12 лет) до 86,2% (в 44 года), а к 60 – 65 годам достигает 100%. Наиболее часто встречающейся патологией пародонта в молодом возрасте является гингивит, после 30 лет – пародонтит.  Обращаемость населения с заболеваниями пародонта существенно возросла за последнее десятилетие и достигла 64% от общего амбулаторного приема. По данным ВОЗ, заболевания пародонта тяжелой степени выявляются у 5-25% взрослого населения, средней степени — у 30-45% и только 2-8% людей имеют здоровые ткани пародонта в возрасте 35-45 лет. Показатели заболеваемости пародонтитом, одной из широко распространенных стоматологических патологий, на сегодняшний день имеют тенденцию к увеличению.   Поражая практически все возрастные группы населения, в том числе молодых людей и даже детей, заболевания пародонта способствуют формированию «нездоровой» нации. Именно они приводят чаще всего к потере зубов вследствие выраженного деструктивного процесса, обусловливая стойкие нарушения функции зубочелюстной системы  Основная часть включает в себя 3 главы. В первой главе проведен анализ современных литературных источников, посвященный изучению структуры заболеваний пародонта. Во второй главе дана полная характеристика материалов и методов исследования. В третьей главе подробно представлены результаты проведенных исследований. Работа содержит 11 таблиц и 8 диаграмм.</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_input_ids = data_module[\"train_dataset\"][0][\"input_ids\"]\n",
    "tokenizer.decode(some_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4648176-a8fb-4011-876d-a9e1941cb6c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:46:14.495281Z",
     "iopub.status.busy": "2024-05-10T12:46:14.494874Z",
     "iopub.status.idle": "2024-05-10T12:46:14.513085Z",
     "shell.execute_reply": "2024-05-10T12:46:14.512370Z",
     "shell.execute_reply.started": "2024-05-10T12:46:14.495261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module[\"train_dataset\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b37e59d-9a7c-4bf7-ac7a-27e954806cd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:45:58.932968Z",
     "iopub.status.busy": "2024-05-10T12:45:58.932534Z",
     "iopub.status.idle": "2024-05-10T12:45:58.945353Z",
     "shell.execute_reply": "2024-05-10T12:45:58.944671Z",
     "shell.execute_reply.started": "2024-05-10T12:45:58.932946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_module[\"train_dataset\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def0f5d-a5b7-40fc-bb9b-3a34bf3b95e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fail with 128k: OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6e396e1-d615-4d7c-8068-cb113ff068ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T01:46:26.377728Z",
     "iopub.status.busy": "2024-05-10T01:46:26.377359Z",
     "iopub.status.idle": "2024-05-10T01:54:37.022502Z",
     "shell.execute_reply": "2024-05-10T01:54:37.021566Z",
     "shell.execute_reply.started": "2024-05-10T01:46:26.377708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 01:46:26,971] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The default cache directory for DeepSpeed Triton autotune, /home/jupyter/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1350 [02:01<45:25:35, 121.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.3633, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1350 [03:41<40:48:17, 108.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.6256, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1350 [05:22<39:26:08, 105.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.8078, 'learning_rate': 3e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1350 [06:37<34:55:48, 93.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0019, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.54 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5d91e48ec5ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1892\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.54 GiB. GPU "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir=training_args.output_dir)\n",
    "\n",
    "print(\"Learnt model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e138a307-18eb-4780-be3a-9ac2709c1e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T01:54:52.953467Z",
     "iopub.status.busy": "2024-05-10T01:54:52.953113Z",
     "iopub.status.idle": "2024-05-10T01:54:53.004382Z",
     "shell.execute_reply": "2024-05-10T01:54:53.003404Z",
     "shell.execute_reply.started": "2024-05-10T01:54:52.953447Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peft_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7c33b45630c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpeft_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'peft_model' is not defined"
     ]
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438d279-3d58-45e0-9b14-6c58892aa76a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Unlucky try: wandb issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3240a8a4-79a9-48be-9dcb-03c57f1a99bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T19:29:07.016411Z",
     "iopub.status.busy": "2024-05-09T19:29:07.015918Z",
     "iopub.status.idle": "2024-05-09T19:38:37.909869Z",
     "shell.execute_reply": "2024-05-09T19:38:37.908380Z",
     "shell.execute_reply.started": "2024-05-09T19:29:07.016389Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin train\n",
      "Parsed arguments\n",
      "Created config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [03:39<00:00, 109.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data_module\n",
      "Prepared model to learn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb.proto.wandb_internal_pb2' has no attribute 'Result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-401c04a19c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-7ec07e0661e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_args, data_args, training_args)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prepared model to learn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mdefault_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_CALLBACKS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_callbacks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdefault_callbacks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         self.callback_handler = CallbackHandler(\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36madd_callback\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0mcb_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WandbCallback requires wandb to be installed. Run `pip install wandb`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtermsetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msdk\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwandb_sdk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_helper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0martifacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArtifact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_alerts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlertLevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/artifacts/artifact.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArtifactCollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArtifactFiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRetryingClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_types\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWBValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/apis/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mreset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvendor_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mApi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mInternalApi\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpublic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mApi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPublicApi\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/apis/internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mApi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mInternalApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mB64MD5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5_file_b64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDIFF_FNAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMETADATA_FNAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgitlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGitRepo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/retry.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckRetryFnType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmailbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContextCancelledError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_MailboxSlot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0m_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0m_event\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36m_MailboxSlot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_MailboxSlot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0m_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0m_event\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb.proto.wandb_internal_pb2' has no attribute 'Result'"
     ]
    }
   ],
   "source": [
    "train(model_args, data_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23add6b-88fb-407e-9dfc-0a5b19392680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
